{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GajTVERvVBqL",
        "outputId": "1d859d36-3079-4964-8f18-82bf46293dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.13 (from langchain_community)\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.13->langchain_community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain_community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.13 langchain-core-0.3.28 langchain_community-0.3.13 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JJts8H94T0OV"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WgUi8qTvT7CY"
      },
      "outputs": [],
      "source": [
        "#Langchain components to use\n",
        "from langchain_community import llms\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "\n",
        "# support for dataset retrieval with hugging face\n",
        "from datasets import load_dataset\n",
        "\n",
        "#with CassIO, the engine powering the Astra DB integration in Langchain\n",
        "#cassio will also initialize the DB connection:\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8_YMPLQU5Ih",
        "outputId": "fb9794a0-e018-4931-8b5c-47342e735493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 #help to read any text inside the pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1jLh2ujyVU5t"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "i5x4ulrZVqlc"
      },
      "outputs": [],
      "source": [
        "## SETUP FOR ASTRA DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_IujsiT-Y3I3"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "ASTRA_DB_ID=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "OPENAI_API_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "s_beJ-USc-ZC"
      },
      "outputs": [],
      "source": [
        "# # First, create the embedding object:\n",
        "# embedding = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_L2fwt5LZWQU"
      },
      "outputs": [],
      "source": [
        "#provide the path to the pdf file\n",
        "pdfreader=PdfReader('query.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "y-0OgR28ZqmP"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Concatenate\n",
        "#read text from pdf\n",
        "raw_text=''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content=page.extract_text()\n",
        "  if content:\n",
        "    raw_text+=content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "7yjBN77JaKrw",
        "outputId": "6722b1cd-f5aa-4f7b-9c7c-251e48bf2330"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'THETHOUSAND BRAINS PROJECT : A N EWPARADIGM FOR\\nSENSORIMOTOR INTELLIGENCE\\nViviane Clay∗\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\nvclay@thousandbrains.org\\nNiels Leadholm*\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\nnleadholm@thousandbrains.orgJeff Hawkins\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\njhawkins@thousandbrains.org\\nDecember 25, 2024\\nABSTRACT\\nArtificial intelligence has advanced rapidly in the last decade, driven primarily by progress in the\\nscale of deep-learning systems. Despite these advances, the creation of intelligent systems that\\ncan operate effectively in diverse, real-world environments remains a significant challenge. In\\nthis white paper, we outline the Thousand Brains Project, an ongoing research effort to develop\\nan alternative, complementary form of AI, derived from the operating principles of the neocortex.\\nWe present an early version of a thousand-brains system, a sensorimotor agent that is uniquely\\nsuited to quickly learn a wide range of tasks and eventually implement any capabilities the human\\nneocortex has. Core to its design is the use of a repeating computational unit, the ‘learning module’,\\nmodeled on the cortical columns found in mammalian brains. Each learning module operates as\\na semi-independent unit that can model entire objects, represents information through spatially\\nstructured reference frames, and both estimates and is able to effect movement in the world. Learning\\nis a quick, associative process, similar to Hebbian learning in the brain, and leverages inductive\\nbiases around the spatial structure of the world to enable rapid and continual learning. Multiple\\nlearning modules can interact with one another both hierarchically and non-hierarchically via a\\ncortical messaging protocol (CMP), creating more abstract representations and supporting multimodal\\nintegration. We outline the key principles motivating the design of thousand-brains systems and\\nprovide details about the implementation of Monty, our first instantiation of such a system. Code\\ncan be found at https://github.com/thousandbrainsproject/tbp.monty , along with more\\ndetailed documentation at https://thousandbrainsproject.readme.io/ .\\nKeywords Sensorimotor ·Neocortex ·Embodied ·General Intelligence ·Reference Frames ·Spatial Representations ·\\nModel-Based ·World Models ·Canonical Microcircuit\\n∗Joint first authors.arXiv:2412.18354v1  [cs.AI]  24 Dec 2024The Thousand Brains Project\\n1 Introduction\\nWe are developing a platform for building AI and robotics\\napplications using the same principles as the human brain,\\na broad research initiative called the Thousand Brains\\nProject. The principles this project builds on are fundamen-\\ntally different from those used in deep learning, currently\\nthe most prevalent form of AI. Therefore, our platform\\nrepresents an alternative form of AI, one that we believe\\nwill play an ever-increasing role in the future.\\nThis paper outlines the motivation of the Thousand Brains\\nProject, as well as the technical details of the underlying\\nalgorithm for sensorimotor intelligence. The aim is to\\nenable developers to build AI applications that are more\\nintelligent, more flexible, and more capable in the many\\napplications that deep learning methods fail. Core to the\\ndesign of thousand-brains systems are the principles laid\\nout in the Thousand Brains Theory [Hawkins et al., 2019],\\na theory of intelligence derived from neuroscientific evi-\\ndence of the anatomy and function of the neocortex. One\\ncore principle of the theory builds on the work of Ver-\\nnon Mountcastle, who argued that the power of the mam-\\nmalian brain lies in its re-use of cortical columns as the\\nprimary computational unit [Mountcastle, 1997, Edelman\\nand Mountcastle, 1978]. In honor of Mountcastle’s idea,\\nwe name the first practical implementation of a thousand\\nbrains system \"Monty\". The code for building and exper-\\nimenting with Monty can be found at https://github.\\ncom/thousandbrainsproject/tbp.monty .\\nOne key differentiator between thousand-brains systems\\nand other AI technologies is that the former are built with\\nembodied, sensorimotor learning at their core. Sensorimo-\\ntor systems learn by sensing different parts of the world\\nover time while interacting with it. For example, as you\\nmove your body, your limbs, and your eyes, the input to\\nyour brain changes. In thousand-brains systems, the learn-\\ning derived from continuous interaction with an environ-\\nment represents the foundational knowledge that supports\\nall other functions. This contrasts with the growing ap-\\nproach that sensorimotor interactions are a sub-problem\\nthat can be solved by beginning with an architecture trained\\non a mixture of internet-scale language and multi-media\\ndata [Driess et al., 2023, OpenAI et al., 2023, Black et al.,\\n2024]. In addition to sensorimotor interaction being the\\ncore basis for learning, the centrality of sensorimotor learn-\\ning manifests in the design choice that all levels of pro-\\ncessing are sensorimotor. As will become clear, sensory\\nand motor processing are not broken up and handled by\\ndistinct architectures, or limited to a single, global action\\noutput [Reed et al., 2022, Driess et al., 2023, Team et al.,\\n2024, Black et al., 2024]. Instead, sensation and motor\\noutputs play a crucial role at every point in thousand-brains\\nsystems where information is processed.\\nA second differentiator is that our sensorimotor systems\\nlearn structured models, using reference frames , explicit\\ncoordinate systems within which locations and rotations\\ncan be represented. Internal models derived from these ref-erence frames keep track of where their sensors are relative\\nto things in the world. Models are learned by assigning\\nsensory observations to locations in reference frames. In\\nthis way, the models learned by sensorimotor systems are\\nstructured, similar to CAD models in a computer. This\\nallows the system to quickly learn the structure of the\\nworld and how to manipulate objects to achieve a variety\\nof goals, what is sometimes referred to as a world model .\\nAs with sensorimotor learning, reference frames are used\\nthroughout all levels of information processing, includ-\\ning the representations of not only environments but also\\nphysical objects and abstract concepts - even the simplest\\nrepresentations are represented within a reference frame.\\nThere are numerous advantages to sensorimotor learning\\nand reference frames. At a high level, you can think about\\nall the ways humans are different from today’s AI. We\\nlearn quickly and continuously, constantly updating our\\nknowledge of the world as we go about our day. We do not\\nhave to undergo a lengthy and expensive training phase\\nto learn something new. We interact with the world and\\nmanipulate tools and objects in sophisticated ways that\\nleverage our knowledge of how things are structured. For\\nexample, we can explore a new app on our phone and\\nquickly figure out what it does and how it works based on\\nother apps we know. We actively test hypotheses to fill in\\nthe gaps in our knowledge. We also learn from multiple\\nmodalities and these different sensory inputs work together\\nseamlessly. For example, we may learn what a new tool\\nlooks like with a few glances and then immediately know\\nhow to grab and interact with the object via touch. Finally,\\nwe carry out complex, planned actions that leverage our\\nknowledge of the world to enable intelligent behavior in\\nnew settings.\\nOne of the most important discoveries about the brain\\nis that most of what we think of as intelligence, from\\nseeing, to touching, to hearing, to conceptual thinking,\\nto language, is enabled by a common neural algorithm\\n[Mountcastle, 1997]. All aspects of intelligence are created\\nby the same sensorimotor mechanism. In the neocortex,\\nthis mechanism is implemented in each of the thousands of\\ncortical columns. This means we can create many different\\ntypes of intelligent systems using a set of common building\\nblocks. The architecture we are creating is built on this\\npremise. Thousand-brains systems will provide the core\\ncomponents and developers will then be able to assemble\\nwidely varying AI and robotics applications using them in\\ndifferent numbers and arrangements. We now elaborate on\\nthe high-level motivations of the Thousands Brains Project\\n(TBP), before describing the technical details of Monty,\\nthe first instance of a thousand-brains system.\\n2 The Thousand Brains Project\\n2.1 Long Term Goals\\nA central long-term goal is to build a universal platform and\\nmessaging protocol for intelligent sensorimotor systems.\\nWe call this protocol the \"Cortical Messaging Protocol\"\\n2The Thousand Brains Project\\n(CMP). The CMP can be used as an interface between dif-\\nferent modules , and its universality is central to the ease of\\nuse of the SDK we are developing. For instance, one per-\\nson may have modules optimized for flying a drone using\\nbirds-eye observations, while another may be working with\\ndifferent sensors and actuators regulating a smart home.\\nDrone operation and smart-home control are quite different\\nsettings, but the modules used in these settings should be\\nable to communicate through the same channels defined\\nhere. Furthermore, a setup for a larger home with multi-\\nple drones might require more modules to fully learn and\\ncontrol the system. The CMP is designed to enable rapid\\nscaling of thousand-brains systems as required. Finally,\\nthird parties could develop sensor modules and learning\\nmodules (terms which we will shortly define) according\\nto their specific requirements, but they would be compati-\\nble with all existing modules due to the shared messaging\\nprotocol.\\nA second goal of the TBP is to be a catalyst for a whole\\nnew way of thinking about machine intelligence. The\\nprinciples of the TBP differ from many principles of pop-\\nular AI methodologies today and are more in line with\\nthe principles of learning in the brain. Most concepts pre-\\nsented here derive from the Thousand Brains Theory (TBT)\\n[Hawkins et al., 2019] and experimental evidence about\\nhow the brain works. Modules in thousand-brains systems\\nare inspired by cortical columns in the neocortex [Edelman\\nand Mountcastle, 1978, Mountcastle, 1997]. The CMP\\nbetween modules relies on object ID and pose information,\\nas could be encoded in neural activity. The communica-\\ntion process is analogous to long-range connections in the\\nneocortex.\\nIn our implementation, we do not need to strictly adhere\\nto all biological details, and it is important to note that\\nshould an engineering solution serve us better for imple-\\nmenting certain aspects, then it is acceptable to deviate\\nfrom the neuroscience. For instance, we do not need to\\nsimulate spikes and can implement the general algorithm\\nto be efficient on today’s hardware. In general, the inner\\nworkings of the modules can can vary in implementation\\ndetail and do not have to rely on neuroscience as long\\nas they adhere to the CMP. However, the core principles\\nof the TBP are motivated by what we have learned from\\nstudying the neocortex. Furthermore, we expect that in\\nmany instances, the TBP will bring together prior work into\\na single framework, including sparsity, active dendrites,\\nsequence memory, and grid cells [Hawkins and Ahmad,\\n2016, Hawkins et al., 2017, Ahmad and Scheinkman, 2019,\\nHawkins et al., 2019, Lewis et al., 2019].\\nFinally, it will be important to showcase the capabilities of\\nour approach. We will work towards creating non-trivial\\ndemos where the implementation showcases capabilities\\nthat would be hard to demonstrate any other way. This may\\nnot be one specific task but could play to the strength of this\\nsystem to tackle a wide variety of tasks. We will also work\\non making Monty an easy-to-use open-source SDK that\\nother practitioners can apply and test on their applications.We want this to be a platform for all kinds of sensorimotor\\napplications and not just a specific technology showcase.\\n2.2 Core Principles\\nWe have a set of guiding principles that steer the Thousand\\nBrains Project. Throughout the life of the project, there\\nmay be several different implementations, and within each\\nimplementation, there may be different versions of the core\\nbuilding blocks, but everything we work on should follow\\nthese core principles:\\n•Sensorimotor learning and inference: We use\\nactively generated temporal sequences of sensory\\nand motor inputs instead of static inputs. The\\noutputs of the system are motor commands.\\n•Modular structure: The same algorithm needs\\nto work for all modalities. This general algorithm\\nembodied in a learning module makes the system\\neasily expandable and scalable.\\n•Cortical Messaging Protocol: The inputs and\\noutputs of a learning module adhere to a defined\\nprotocol such that many different sensor modules\\n(and modalities) and learning modules can work\\ntogether seamlessly.\\n•Voting: A mechanism by which a collection of\\nexperts can use different information and mod-\\nels to come to a faster, more robust and stable\\nconclusion.\\n•Reference frames: The learned models should\\nhave inductive biases that make them naturally\\ngood at modeling a structured world that evolves\\nover time. The learned models can be used for a\\nvariety of tasks such as manipulation, planning,\\nimagining previously unseen states of the world,\\nfast learning, generalization, and many more.\\n•Rapid, continual learning where learning and\\ninference are closely intertwined: Supported by\\nsensorimotor embodiment and reference frames,\\nbiologically plausible learning mechanisms en-\\nable rapid knowledge accumulation and updates\\nto stored representations while remaining robust\\nunder the setting of continual learning. There\\nis also no clear distinction between learning and\\ninference. We are always learning, and always\\nperforming inference.\\n•Model-free and model-based policies: Low-\\nlevel, model-free policies provide efficient means\\nof interacting with the world, but are crucially\\ncombined with model-based policies that support\\nflexible action planning in novel situations.\\nIn the initial implementation presented here, many compo-\\nnents are deliberately notbiologically constrained, and/or\\nsimplified, so as to support visualizing, debugging, and\\nunderstanding the system as a whole. For example, ob-\\nject models are currently based on explicit graphs in 3D\\nCartesian space. In the future, these elements may be sub-\\n3The Thousand Brains Project\\nstituted with more powerful, albeit more inscrutable neural\\ncomponents.\\n2.3 Challenging Preconceptions\\nSeveral of the ideas and ways of thinking introduced in\\nthis document may be counter-intuitive to those familiar\\nwith current AI methods, including deep learning. For\\nexample, ideas about intelligent systems, learning, models,\\nhierarchical processing, or action policies that you already\\nhave in mind might not apply to the system that we are\\ndescribing. We therefore ask the reader to try and dispense\\nwith as many preconceptions as possible and to understand\\nthe ideas presented here on their own terms. We are happy\\nto discuss any questions or thoughts that may arise from\\nreading this document. Please join our Discourse forum or\\nreach out to us at info@thousandbrains.org.\\nBelow, we highlight some of the most important differ-\\nences between the system we are building and other AI\\nsystems.\\n•We are building a sensorimotor system. It learns\\nby interacting with the world and sensing differ-\\nent parts of it over time. It does not learn from a\\nstatic dataset . This is a fundamentally different\\nway of learning than most leading AI systems\\ntoday and addresses a (partially overlapping) dif-\\nferent set of problems.\\n•We will introduce learning modules as the basic,\\nrepeatable modeling unit, comparable to a cortical\\ncolumn. An important detail to point out here is\\nthat none of these modeling units receives the full\\nsensory input. For example, in vision, there is\\nno ‘full image’ anywhere . Each sensor senses\\na small patch in the world. This is in contrast to\\nmany AI systems today, where all sensory input\\nis fed into a single model.\\n•Despite the previous point, each modeling sys-\\ntem can learn complete models of objects and\\nrecognize them on its own. A single modeling\\nunit should be able to perform all basic tasks\\nof object recognition and manipulation . Us-\\ning more modeling units makes the system faster\\nand more efficient and supports compositional\\nand abstract representations, but a single learning\\nmodule is itself a powerful system. In the sin-\\ngle module scenario, inference always requires\\nmovement to collect a series of observations, in\\nthe same way that recognizing a coffee cup with\\none finger requires moving across its surface.\\n•All models are structured by reference frames.\\nAn object is not just a bag of features. It is a\\ncollection of features at locations. The relative\\nlocations of features to each other are more\\nimportant than the features themselves . These\\nprinciples are used for modeling all discrete con-\\ncepts in the world, from the simplest of physicalobjects to abstract concepts in society or mathe-\\nmatics.\\n•Action policies are, first and foremost, model-\\nbased . Learned models of objects in the world\\nare used to determine appropriate actions in novel\\nsituations. Any given learning module can use its\\ninternal models to propose goal-states that are ei-\\nther decomposed into simpler goal-states in other\\nlearning modules, or are acted upon directly by\\nmotor systems. In this way, complex policies can\\nbe hierarchically decomposed, while still lever-\\naging learned models. Over time and with prac-\\ntice, model-based policies become more efficient,\\nwhile model-free policies can learn to do certain\\ntasks rapidly and with finesse, but model-free\\npolicies are not the initial basis of actions in unfa-\\nmiliar settings.\\n•Motor output can be generated at any level\\nof the system. In contrast to many current ap-\\nproaches for sensorimotor interaction, we do not\\nhave a separate hierarchy of sensory processing\\nfollowed by the generation of motor commands.\\nInstead, each learning module, even at the lowest\\nsensory level, produces action outputs. This is\\nanalogous to the projections to subcortical motor\\nregions found in every area of the neocortex, even\\nregions classically thought of as sensory regions.\\n2.4 Capabilities of the System\\nThe thousand-brains architecture is designed to be a\\ngeneral-purpose AI system. It is not designed to solve\\na specific task or set of tasks. Instead, it is designed to be\\na platform that can be used to build a wide variety of AI\\napplications. Like an operating system or a programming\\nlanguage does not define what the user applies it to, the\\nThousand Brains Project will provide the tools necessary\\nto solve many of today’s current problems as well as com-\\npletely new and unanticipated ones without being specific\\nto any one of them.\\nEven though we cannot predict the ultimate use cases of\\nthe system, we want to test it on a variety of tasks and keep\\na set of capabilities in mind when designing the system.\\nThe basic principle here is that it should be able to solve\\nany task the neocortex can solve. If we come up with a\\nnew mechanism that makes it fundamentally impossible\\nto do something the neocortex can do, we need to rethink\\nthe mechanism. For example, thousand-brains systems\\nshould be able to model the world through any kind of\\nmovement-based sensory modality, from touch to echolo-\\ncation. They should also be able to conceptualize abstract\\nspaces, execute a series of intricate movements, and plan\\nlong-term actions. However, tasks such as multiplying\\narbitrary large numbers, or predicting the structure of a\\nprotein given its genetic sequence, are domains much bet-\\nter left to alternative technologies, such as calculators or\\ndeep-learning.\\n4The Thousand Brains Project\\nThe following is a list of capabilities that we always con-\\nsider when designing and implementing the system. We\\nare not looking for point solutions for each of these prob-\\nlems but a general algorithm that can solve all of them. It\\nis by no means a comprehensive list, but it should give an\\nidea of the scope of the system.\\n•Recognizing objects independent of their location\\nand orientation in the world.\\n•Determining the location and orientation of an\\nobject relative to the observer or to another object\\nin the world.\\n•Recognizing an object and its pose by moving\\none sensor over the object.\\n•Performing flash inference (inference with no\\nmovement) by using many sensors in tandem.\\n•Performing learning and inference under noisy\\nconditions.\\n• Learning from a small number of samples.\\n•Learning from continuous interaction with the\\nenvironment, maintaining previously learned rep-\\nresentations.\\n• Learning without explicit supervision.\\n•Recognizing objects when other objects partially\\nocclude them.\\n•Learning categories of objects and generalizing\\nto new instances of a category.\\n•Learning and recognizing compositional objects,\\nincluding novel combinations of their parts.\\n•Recognizing objects subject to novel deforma-\\ntions (e.g. Dali’s ‘melting clocks’, a crumpled up\\nt-shirt, or recognizing objects learned in 3D but\\nseen in 2D).\\n•Recognizing an object independent of its scale,\\nand estimating its scale.\\n•Modeling and recognizing object states and be-\\nhaviors (e.g. if a stapler is open or closed; whether\\na person is walking or running, and how their\\nbody evolves over time under these conditions).\\n•Using learned models to alter the world and\\nachieve goals, including goals that require de-\\ncomposition into simpler tasks.\\n•Generalizing modeling to abstract concepts de-\\nrived from concrete models.\\n•Modeling language and associating it with\\ngrounded models of the world.\\n• Modeling other entities (Theory of Mind).\\n3 Overview Of The Architecture\\nThere are three major components that play a role in the\\narchitecture: sensor modules, learning modules, and themotor system. These three elements are tied together by a\\nfinal key component, a common communication protocol.\\nDue to this unified messaging protocol, the inner workings\\nof each individual component can be quite varied as long\\nas they have the appropriate interfaces. A simple example\\nof a sensor module coupled to a learning module is shown\\nin Figure 1, although we will begin by describing the CMP.\\nFigure 1: Sensor modules receive and process the raw\\nsensory and motor input. This is then communicated via a\\ncommon messaging protocol to a learning module which\\nuses this information to learn and recognize models of\\nanything in the environment.\\n3.1 Cortical Messaging Protocol\\nWe use a common communication protocol that all compo-\\nnents - learning modules, sensor modules, and the motor\\nsystem - use to share information. By defining a consistent\\ninformation format that sensor modules and learning mod-\\nules must output, and that motor systems must receive, it\\nis possible for all components to communicate with each\\nother, and to combine them arbitrarily. Due to its inspira-\\ntion from long-range connections in the cortex, we call this\\ncommon communication protocol the Cortical Messaging\\nProtocol (CMP)\\nIn order to define the CMP, we must first define what we\\nmean by an object, or a feature, in Monty. An object is\\na discrete entity composed of a collection of one or more\\nother objects, each with their own associated pose. For ex-\\nample, an apple at a location and orientation in space is an\\nobject, but equally, an object could be a scene, an abstract\\narrangement of concepts, or any other composition of sub-\\nobjects. At the lowest level of this object hierarchy, an\\nobject is composed of inputs from a sensor module, which\\nare also discrete entities with a location and orientation in\\nspace. Sensor inputs play a similar role to objects at other\\n5The Thousand Brains Project\\npoints in a hierarchy of learning modules, but the ‘objects’\\ndetected by sensors cannot be further decomposed. Wher-\\never an object is being processed by a component of the\\nsystem, it can also be referred to as a feature . By conven-\\ntion, we usually refer to the input of a learning module\\nas features and the output as an object ID. However, the\\nobject ID output of one learning module can become the\\nfeature input to the next learning module so they are by\\ndefinition the same.\\nAt its core, a CMP-compliant output contains a feature at\\na pose . The pose contains a location in 3D space (naturally\\nincluding 1D or 2D space) representing where the sensed\\nfeature is relative to the body, or another common reference\\npoint such as a landmark in the environment. In addition to\\nlocation, the pose includes information about the feature’s\\n3D orientation, which could be defined by the direction\\nof a surface’s point normal and its principal curvature, or\\nthe orientation of an object. Importantly, the message may\\ncontain additional feature information, such as color, the\\nmagnitude of sensed curvature, or an object ID. Counter-\\nintuitively, the nature of the feature does not need to be\\nspecified in the message for it to be a valid signal.\\nWe highlight the choice that non-pose attributes of a fea-\\nture are optional. This is contrary to many existing AI\\nsystems where models are often closer to bags-of-features\\nand object structure is weakly represented, if at all. Here,\\nthe relative locations of features are more important than\\nthe features themselves. An example of how this aligns\\nwith human perception is how fruits arranged in the shape\\nof a face can be easily recognized as a face, even though\\nno typical face \"features\" are present. On the other hand,\\nhumans would not classify a random arrangement of eyes,\\na nose, and lips as a face.\\nBesides features and their poses, the standardized message\\nalso includes information about the sender’s ID (e.g. the\\nparticular sensor module) and a confidence rating. Fur-\\nther below we discuss the internal models that learning-\\nmodules (LMs) develop - importantly, the CMP is never\\nused to share these models between LMs. Instead, it can\\nonly communicate more abstract information about these\\nmodels (such as an object ID). The inputs and outputs of\\nthe system (raw sensory input to the SM and motor com-\\nmand outputs from motor modules) can have any format\\nand do not adhere to any messaging protocol. They are\\nspecific to the agents sensors and actuators and represent\\nthe systems interface with the environment.Fin a common\\nreference frame (e.g. relative to the body2). This makes it\\npossible for all components to meaningfully interpret the\\npose information they receive.\\n2In the following sections we may call this common reference\\nframe \"body-centric\" . In general, we just mean a common refer-\\nence frame for all sensors. There may be applications without a\\nconcrete body (like several cameras set up in different locations\\nof a room, a swarm of agents, or an agent navigating a more ab-\\nstract space like the internet) where this just refers to an arbitrary\\npoint in space that all communicated poses are relative to.3.2 Sensor Modules\\nThousand-brains systems can work with any type of sensor\\n(vision, touch, radar, LiDAR,...) and integrate information\\nfrom multiple sensory modalities without effort. For this\\nto work, sensor modules need to communicate information\\nin a common language. Transforming raw sensory input\\ninto this common language is the job of the sensor module.\\nEach sensor module receives information from a small\\nsensory patch as input. This is analogous to a small patch\\non the retina, or a patch of skin, or the pressure information\\nat one whisker of a mouse. In the simplest architecture, one\\nsensor module sends information to one learning module,\\nwhich models this information. How such local sensory\\ninputs are integrated across time and space will be covered\\nin a moment when we discuss learning modules.\\nThe information processing within the sensor module turns\\nthe raw information from the incoming sensor patch into\\nthe cortical messaging protocol (detailed in section 3.1).\\nThis process can be compared to light hitting the retina\\nand being converted into spikes, the output of biological\\nneurons. Additionally, the pose of the feature relative to\\nthe body is calculated from the feature’s pose relative to the\\nsensor and the sensor’s pose relative to the body. As such,\\neach sensor module outputs the feature it senses, as well as\\nthe feature’s pose (location and rotation) in body-centric\\ncoordinates. The availability of this pose information is\\ncentral to how the thousand-brains architecture operates.\\nA general principle of the system is that any processing\\nspecific to a modality happens in the sensor module. The\\noutput of the sensor module is not modality-specific any-\\nmore and can be processed by any learning module. A\\ncrucial requirement here is that each sensor module knows\\nthe pose of the feature relative to the sensor. This means\\nthat sensors need to be able to detect features and poses of\\nfeatures. They also need to be able to keep track of their\\nposition in space. This could be directly provided from the\\nsystem, inferred from sensory inputs (like optical flow), or\\ncalculated from efference copies of motor commands.\\n3.3 Learning Modules\\nThe basic building block for sensorimotor processing and\\nmodeling is the learning module (LM). These are repeating\\nelements, each using the same input and output interface.\\nEach LM should function as a stand-alone unit and be able\\nto recognize objects on its own. Combining multiple LMs\\ncan speed up recognition (e.g. recognizing a cup using five\\nfingers vs. one), allows for LMs to focus on storing only\\nsome objects, and enables learning compositional objects.\\nLMs receive features at poses. Features can either be\\nfeature IDs from a sensor module or object IDs (also in-\\nterpreted as features) from a lower-level LM. The feature\\nor object representation might be in the form of a discrete\\nID (e.g. the color red, a cylinder), or could be represented\\nin a more high dimensional space (e.g. a vector of binary\\nvalues representing hue, or corresponding to a fork-like\\n6The Thousand Brains Project\\nFigure 2: Learning modules learn structured models\\nthrough sensorimotor interaction, using reference frames.\\nThey model how incoming features are arranged relative\\nto each other in space.\\nobject). Additionally, LMs receive the feature’s or object’s\\npose relative to the body, where the pose includes location\\nand rotation. In this way, body-centric coordinates serve\\nas a common reference frame for spatial computations, as\\nopposed to the pose of features relative to each individual\\nsensor. From this information, higher-level LMs can build\\nup structured models of compositional objects (e.g. large\\nobjects or scenes).\\nThe features and relative poses are incorporated into a\\nmodel of the object. All models have an inductive bias\\ntowards learning objects within a 3-dimensional space,\\ncomplimented by a temporal dimension. When interacting\\nwith the physical world, the 3D inductive bias is used to\\nplace features in internal models accordingly. However,\\nthe exact structure of space can potentially be learned,\\nsuch that the lower-dimensional space of a melody, or the\\nabstract space of a family tree, can be represented.\\nThe LM, therefore, encompasses two major principles of\\nthe TBT: sensorimotor learning, and building models using\\nreference frames (see Figure 2). Both ideas are motivated\\nby studies of cortical columns in the neocortex (see Figure\\n3), as well as Hawkins et al. [2017, 2019].\\nBesides learning new models, the LM also tries to match\\nthe observed features and relative poses to already learned\\nmodels stored in memory. Internally, LMs use displace-\\nments between consecutive poses and map them into the\\nmodel’s reference frame. This makes it possible to detect\\nobjects even at novel poses.To generate the LM’s output , we need to get the pose of\\nthe sensed object relative to the body. We can calculate\\nthis from the current incoming pose (pose of the sensed\\nfeature relative to the body) and the poses stored in the\\nmodel of the object. This pose of the object can then be\\npassed hierarchically to another LM in the same format as\\nthe sensory input (features at a pose relative to the body\\nwhere the feature is the inferred object ID).\\nOnce the LM has determined an object’s ID and pose, it\\ncan use the most recent observations (and possibly collect\\nmore) to update its model. As such, LMs continually learn\\nmore about the world, and learning and inference are two\\nclosely intertwined processes.\\n3.4 Motor Information and Action Policies\\nMovement is central to how thousand-brains systems un-\\nderstand the world. The spatial nature of reference frames\\nis dependent on integrating movement information so that\\na learning module knows where its input features are lo-\\ncated at any given moment. The movement information\\n(pose displacement) can be a copy of the selected action\\ncommand (efference copy) or deduced from the sensory in-\\nput. Without the efference copy, movement can be detected\\nfrom information such as optical flow or proprioception.\\nSensor modules use movement information to update their\\npose relative to the body. LMs use it to update the hypothe-\\nsized location of their incoming features within an object’s\\nreference frame.\\nWhile movement is clearly important for an LM to un-\\nderstand the outside world, it is also important that this\\nmovement is not random. What’s more, an intelligent sys-\\ntem should be able to exert influence on the external world.\\nThis is where policies become crucial.\\nThousand-brains systems make use of a combination of\\nmodel-free policies, corresponding to lower-level compo-\\nnents of the system (sensor-module - motor-system loops),\\ntogether with model-based policies based within LMs and\\nusing learned models to inform actions.\\nModel-based policies use more computational resources\\nto enable more principled movement, such as moving a\\nsensor to a location that will minimize uncertainty about\\nthe currently observed object. These policies are derived\\nfrom LMs, where each LM produces a motor output, anal-\\nogous to the universal motor outputs found in cortical\\ncolumns [Prasad et al., 2020]. The motor output is for-\\nmalized as a goal state and also adheres to the CMP. The\\ngoal state could, for example, use the learned models and\\ncurrent hypotheses to calculate a sensor state that resolves\\nuncertainty about which of two possible objects is being\\nobserved. It can also help to guide directed and more ef-\\nficient exploration of parts of objects that are currently\\nunderrepresented in the internal models. Different poli-\\ncies can be leveraged depending on whether we are trying\\nto recognize an object or trying to learn new information\\nabout an object. Finally, policies can enable a learning\\n7The Thousand Brains Project\\nFigure 3: Conceptual sketch of how the learning module could be implementing possible mechanisms of cortical\\ncolumns. The figure on the right represents three cortical columns, including cellular layers. The internal structure of a\\nlearning module can be mapped onto these layers.\\nmodule to change the state of the world, such as pushing a\\nbutton, or changing the position of an object on a table.\\nHierarchy can also be leveraged for goal-states, where a\\nmore abstract goal-state in a high-level LM can be achieved\\nby decomposing it into simpler goal-states for lower-level\\nLMs. Importantly, the same LMs that learn models of ob-\\njects are used to generate goal-states, enabling hierarchical,\\nmodel-based policies.\\nModel-free policies are useful for purely sensory-based ac-\\ntions such as smoothly moving a sensor over the surface of\\nan object, or attending to a prominent feature. Model-free\\npolicies can also learn to carry out frequently performed\\ntasks in a dexterous and rapid manner, freeing computa-\\ntional resources required for model-based policies. Finally,\\ngoal-states generated by LMs must be transformed into\\nmotor commands for actuators - a process that recruits\\nmodel-free policies (innate or learned) in the motor sys-\\ntem.\\nIn the brain, much of this processing occurs subcortically.\\nIn a thousand-brains system, this corresponds to the motor-\\nsystem area. We note that the motor area does not know\\nabout models of objects that are learned in the LMs, and\\ntherefore needs to receive useful goal states from the LMs.\\nThese commands adhere to the CMP, but the outputs of\\nthe motor area will deviate from the protocol in order to\\ninterface with the actuators of the system. This means the\\nmotor system serves the reverse role of the sensor module,\\ntranslating CMP-compliant goal states into the specific\\nmovement commands of the actuator it is connected to.\\n3.5 Multi-LM Systems\\nAny given Monty system can be composed of multiple\\nlearning modules. Depending on their arrangement, LMsinteract with one-another in a hierarchical manner, or via\\nvoting. A brief overview of these concepts is given below,\\nwhile these possibilities are shown visually in Figure 4.\\n3.5.1 Hierarchy: Composition and Learning on\\nDifferent Spatial Scales\\nLearning modules can be stacked in a hierarchical fashion\\nto process larger input patches and higher-level concepts.\\nA higher-level LM receives feature and pose information\\nfrom the output of a lower-level module and/or from a\\nsensor patch with a larger receptive field, mirroring the\\nconnectivity of the cortex. The lower-level LM never sees\\nthe entire object it is modeling at once but infers it either\\nthrough multiple consecutive movements and/or voting\\nwith other modules. The higher-level LM can then use the\\nrecognized model ID as a feature in its own models. This\\nmakes it more efficient to learn larger and more complex\\nmodels as we do not need to represent all object details\\nwithin one model. In particular, this enables the repre-\\nsentation of compositional objects by quickly associating\\ndifferent object parts with each other as relative features\\nin a higher-level model. We discuss the importance of\\ncomposition more later.\\n3.5.2 Voting: Rapid Consensus\\nLMs share lateral connections in order to communicate\\ntheir estimates of the current object ID and pose. This\\nprocess, which we term voting , adheres to the CMP, pass-\\ning feature-pose information. Unlike connections between\\nlower and higher LMs however, voting communicates a set\\nof all possible objects and poses under the current evidence\\n(i.e. multiple messages adhering to the CMP). Through the\\nlateral voting connections, LMs attempt to reach a consen-\\nsus on which object they are sensing at the moment and its\\n8The Thousand Brains Project\\nFigure 4: By using a common messaging protocol between sensor modules and learning modules, the system can easily\\nbe scaled in multiple dimensions. This provides a straightforward way for dealing with multiple sensory modalities.\\nUsing multiple learning modules next to each other can improve robustness and speed through votes between them.\\nAdditionally, stacking learning modules on top of each other allows for more complex, hierarchical processing of inputs\\nand modeling compositional objects.\\n9The Thousand Brains Project\\npose. This helps to recognize objects faster than a single\\nmodule could.\\nWe earlier highlighted that CMP messages are encoded in a\\ncommon reference frame. This is key for voting to account\\nfor the relative displacement of sensors and, therefore,\\nlocations within LM models. For example, when two\\nfingers touch a coffee mug in two different parts, one might\\nsense the rim, while the other senses the handle. As such,\\n‘coffee mug’ will be in both of their working hypotheses\\nabout the current object. When voting, they do not simply\\ncommunicate ‘coffee mug’, but also where on the coffee\\nmug other LMs should be sensing it, according to their\\nrelative displacements. As a result, voting is not simply a\\n‘bag-of-features’ operation but is dependent on the relative\\narrangement of features in the world.\\nNote that votes sent via the CMP do not contain any infor-\\nmation about the input features received by that LM. For\\nexample, an LM might receive point-normals and surface\\ncurvature as its input features from an SM, and use this to\\nmodel objects like coffee mugs and staplers. During voting,\\nit will communicate its hypotheses around coffee mugs and\\nstaplers, but it will not communicate any information about\\nsensed curvature to other learning modules.\\nFinally, the CMP is independent of modality, and as such,\\nLMs that have learned objects in different modalities (e.g.\\nvision and touch), can still vote with each other to quickly\\nreach a consensus. This voting process is inspired by the\\nvoting process described in Hawkins et al. [2017].\\n3.6 Bringing it Together\\nTo consolidate these concepts, please see Figure 5 for an\\nexample instantiation of the system in a concrete setting.\\nIn this example, we see how the system could be applied\\nto sensing and recognizing objects and scenes in a 3D\\nenvironment using several different sensors, in this case,\\ntouch and vision.\\nWhile provided to make the key concepts described above\\nmore concrete, bear in mind that this represents only one\\nexample of how the architecture can be instantiated. By\\ndesign, thousand-brain systems can be applied to any ap-\\nplication that involves sensing and active interaction with\\nan environment. Indeed, this might include more abstract\\nexamples such as browsing the web, or interacting with\\nthe instruments that control a scientific experiment.\\n4 Implementation\\nWe now describe in further detail the implementation of\\nMonty, the first instance of a thousand-brains system. As\\njust outlined in the general case, the basic components of\\nMonty are: sensor modules (SM) to turn raw sensory data\\ninto a common language; learning modules (LM) to model\\nincoming streams of data and use these models for inter-\\nacting with the environment; motor system(s) to carry out\\nactions, and translate abstract motor commands from thelearning module into a format for actuators; and an environ-\\nment in which the system is embedded and which it tries\\nto model and interact with. The components within the\\nMonty are connected by the Cortical Messaging Protocol\\n(CMP) so that basic building blocks can be easily repeated\\nand stacked on top of one another. Any communication\\nwithin Monty is expressed as features at poses relative to a\\ncommon reference frame such as the body. These CMP-\\ncompliant signals can be interpreted in different ways. For\\nexample, pose to the motor system is a target to move to,\\npose to an LM is the most likely sensed (from SM) or\\ninferred (from LM) pose, and poses via voting connections\\nare possible poses.\\nAll of these elements are implemented in Python\\nathttps://github.com/thousandbrainsproject/\\ntbp.monty and their algorithmic details are described in\\nthe following sections. We begin by going into detail about\\ngeneral concepts, such as the experimental environment\\nwe currently employ, before describing the specifics of sen-\\nsor modules, learning modules, and finally, action policies.\\nThis algorithm is under active development, and for a more\\ndetailed and the most up-to-date description, please refer\\nto our documentation.\\nNote that these descriptions refer to our current implemen-\\ntation and there will likely be many other implementations\\nof the different components in the future. The idea of this\\nframework is that any component can be customized and\\nreplaced as long as it follows the defined interface. For\\ninstance, one can switch out the type of learning module\\nused without changing the sensor modules, environment,\\nor motor system. Alternatively, one could implement a\\nsensor module for a specific sensor and plug this into an\\nexisting learning module. Yet another possibility would\\nbe to test the current Monty configuration in another sen-\\nsorimotor environment. The possibilities are endless, and\\nthe specific configuration and testbed described below is\\nsimply one instantiation that we found useful for designing\\nthis system.\\n5 Experimental Evaluations\\nThe testbed currently used most often is focused on ob-\\nject recognition. While this involves learning models of\\nobjects and interacting with the environment, it all serves\\nthe purpose of recognizing objects and their poses. In the\\nfuture, this focus will shift to settings where object pose\\nand ID recognition subserves more complex interactions\\nwith the environment, such as manipulating the world to\\nreach certain goal-states.\\nDuring an experiment, an agent collects a sequence of ob-\\nservations by interacting with an environment. We distin-\\nguish between training (internal models are updated using\\nthis sequence of observations) and evaluation (the agent\\nonly performs inference using already learned models but\\ndoes not update them). In practice, Monty will always\\nbe learning, but establishing a distinct evaluation phase is\\nuseful for benchmarking performance in a controlled way.\\n10The Thousand Brains Project\\nFigure 5: High-level overview of the architecture with all the main conceptual components mirroring Figure 4 applied\\nto a concrete example. Blue lines indicate the feed-forward flow of information up the hierarchy. Purple lines show\\ntop-down connections, biasing the lower-level LMs. Green lines show lateral voting connections. Pink lines show\\nthe communication of goal states, which eventually translate into motor commands in the motor system. Every LM\\nhas a direct motor output. Information communicated along solid lines follows the CMP (contains features and pose).\\nDiscontinuations in the diagram are marked with dots on line ends. Dashed lines are the interface of the system with\\nthe world and do not need to follow the CMP. Blue dashed lines communicate raw sensory input from sensors. Pink\\ndashed lines communicate motor commands to the actuators. The large, semi-transparent blue arrow is an example of a\\nconnection carrying sensory outputs from a larger receptive field directly to a higher-level LM.\\n11The Thousand Brains Project\\nFor practical purposes, time is divided into discrete steps.\\nWe also divide an experiment into multiple episodes and\\nepochs for easier measurement of performance. Overall,\\nwe discretize time in the three ways listed below.\\n•Step: Taking one action and receiving one obser-\\nvation. A step can happen at the level of Monty, as\\nwell as at the level of individual learning modules.\\nThe former includes steps that are deemed irrele-\\nvant for learning modules and where information\\nis not sent to them, such as due to minimally\\nchanging inputs.\\n•Episode: Putting a single object in the environ-\\nment and taking steps until a terminal condition is\\nreached, like recognizing the object or exceeding\\nthe maximum number of permitted steps.\\n•Epoch: Running one episode on each ob-\\nject/scene in the training or evaluation set of ob-\\njects/scenes.\\n6 Environment and Agent\\nThe 3D environment and simulation engine used for most\\nexperiments is Habitat [Savva et al., 2019, Szot et al., 2021,\\nPuig et al., 2023]. An agent is an actuator that can move\\nindependently in the environment, and has sensors coupled\\nto it. Environments are currently initialized with one agent\\nthat has Nsensors attached to it. For most experiments,\\ntwo sensors are used: the first sensor is the sensor patch\\nwhich is used for learning. It is a camera zoomed in 10x so\\nthat it can only perceive a small patch of the environment.\\nThe second sensor is a view-finder, which is at the same\\nlocation as the patch and moves together with it, but its\\ncamera is not zoomed in. The view-finder is only used at\\nthe beginning of an episode to get a good view of the object\\nand for visualization, but not for learning or inference\\n(more details are in the discussion of policies found in\\nSection 11). The agent setup can also be customized to\\nuse more than one sensor patch, such as the five patches in\\nFigure 7).\\nOne can also initialize multiple agents (each with multiple\\nsensors) and connect them to the same Monty instance.\\nThe difference between adding more agents vs. adding\\nmore sensors to the same agent is that all sensors connected\\nto one agent move together, like neighboring patches on\\nthe retina. On the other hand, separate agents can move\\nindependently, like fingers on a hand (see Figure 8).\\nThe environment we typically evaluate object and pose\\ndetection in is an empty space with one object, although\\nwe are beginning to experiment with multiple objects. The\\nobject can be initialized in different rotations, positions\\nand scales, although we do not currently vary the latter.\\nFor objects, one can either use the default Habitat objects\\n(cube, sphere, capsule, etc.) or the YCB object dataset\\n[Calli et al., 2015], containing 77 more complex objects\\nsuch as a cup, bowl, chain, or hammer, as shown in Figure\\n9. Currently there is no physics simulation so objects are\\nnot affected by gravity or touch and therefore do not move.Of course, other datasets and objects can be used, and\\nindeed we are not limited to 3D environments. For\\nexample, one data configuration lets agent move in\\n2D over images from the Omniglot dataset or photos\\nfrom an RGBD camera. The only crucial requirement\\nis that we can use an action to retrieve a new, action-\\ndependent, observation from which we can extract a\\npose. Finally, we have implemented a simple dataset\\nwith physics-dependent objects that evolve over time,\\nalthough we have not yet begun testing Monty in\\nthis setting. For a full list of current environments see\\nhttps://thousandbrainsproject.readme.io/docs/environment-\\nagent.\\n7 The Monty Architecture\\nThe Monty architecture contains everything needed to\\nmodel the environment and interact with it. It consists of\\nsensor modules and learning modules, the communications\\nwiring between them, and a motor system for carrying out\\nactions.\\nIts specification consists of:\\n•A list of SMs, each of which is responsible for\\nprocessing raw sensory input and transforming it\\ninto a canonical format that any LM can operate\\non.\\n•A list of LMs, each of which is responsible for\\nbuilding models of objects given outputs from a\\nsensor module.\\n•The mapping describing the precise coupling be-\\ntween SMs and LMs\\n•The mapping describing the precise coupling be-\\ntween LMs (for hierarchical and voting opera-\\ntions).\\n•A motor system responsible for moving the\\nagent(s) of the system. This might also be imple-\\nmented as motor modules, akin to sensor module.\\n• The mapping of sensors to an associated agent.\\nUsing the above information, we can specify the precise\\nstructure underlying an instance of Monty. For instance, if\\nwe have five sensors in the environment, we would specify\\nfive sensor modules, each corresponding to one sensor.\\nEach sensor module could be connected to one learning\\nmodule and the association between the learning modules\\nis specified in an LM-LM connectivity matrix (specifying\\nboth lateral and hierarchical connectivity). This particular\\nsystem’s architecture would then look as shown in Figure\\n10.\\n8 Observations and Sensor Modules\\nThe universal format that all sensor modules output is the\\nCMP-compliant features at a pose in 3D space. Each\\nsensor connects to a sensor module which turns the raw\\nsensory information into this format for down-stream pro-\\ncessing. Each input to an LM therefore contains x, y, z\\n12The Thousand Brains Project\\nFigure 6: Three ways time is discretized in our current experimental setup is into steps (one movement and one\\nobservation), episodes (take as many steps as needed to reach the terminal condition of the environment such as\\nrecognizing an object or completing a task), and epoch (cycle through all objects/scenarios in the dataset once). In this\\nexample, a red mug is observed in the first episode, and a two-tone painted cylinder is observed in the jth episode.\\ncoordinates of the feature location relative to the body,\\nand three orthonormal vectors indicating its rotation. In\\nsensor modules processing visual and tactile information,\\nthese pose-defining vectors are defined by the point nor-\\nmal and principal curvature directions sensed at the center\\nof the patch. In learning modules (as detailed later), the\\npose vectors are defined by the detected rotation of an ob-\\nject. Additionally, the sensor module returns any sensed\\npose-independent features (e.g. color, texture, or curvature\\nmagnitude). The sensed features can be modality-specific\\n(e.g., color for vision or temperature for touch), while the\\npose is modality-agnostic.\\nA CMP-compliant message must contain the following\\ninformation:\\n•Location (relative to the body or another common\\nreference frame, such as a prominent feature in\\nthe environment)\\n•Morphological features: 3x3 orthonormal vectors\\ndefining the orientation of the sensed feature.\\n•Non-morphological features: color, texture, cur-\\nvature, etc.\\n• ‘Confidence’ (defined in the range [0, 1]).\\n•A boolean for whether the message should be\\nused (sent or processed downstream).\\n•Sender ID (a unique string identifying the sender)\\n- this is used in an analogous way to the anatomi-\\ncal wiring found in the brain.\\n•Sender type (whether the sender is a sensor mod-\\nule or learning module).\\nA CMP-message is quite versatile, and depending on what\\nsystem outputs it, it can be interpreted in different ways.\\nOutput by a sensor module, it can be seen as the observed\\nfeatures. When output by the learning module, it can be\\ninterpreted as the hypothesized or most likely object. As amotor output of an LM, it can be seen as a goal state (for\\ninstance, specifying the desired location and orientation\\nof a sensor or object in the world). Lastly, when sent as\\nlateral votes between LMs, it is interpreted as all possible\\nobjects and poses.\\nNote that some features are extracted using all of the infor-\\nmation in a sensor patch (e.g. the locations of all points in\\nthe patch are used for point-normal and curvature calcula-\\ntion) but then refer to the center of the patch (e.g. only the\\ncurvature and point normal of the center are returned). At\\nthe moment, all the feature extraction is predefined, but in\\nthe future, some low-level feature extraction will likely be\\nlearned.\\n9 Learning Modules\\nLearning modules are the core modeling unit of Monty and\\nare where all structured representations are learned. They\\nare responsible for learning models from the incoming\\nsensorimotor data, which they receive either from sensor\\nmodules, or other learning modules. Their input and output\\nformats are features at a pose, and therefore comply with\\nthe CMP. Using the displacement between two consecutive\\ninputs, they can learn object models of features relative to\\neach other and recognize objects that they already know,\\nindependent of where they are in the world. How exactly\\nthis happens is determined by the details of the learning\\nmodule - many possible architectures can be leveraged, al-\\nthough we will focus our description on the first generation\\nof LMs that we have implemented.\\nGenerally, each learning module contains a buffer, which\\nfunctions as a short-term memory, and some form of long-\\nterm memory that stores models of objects. Both can then\\nbe used to generate hypotheses about what is currently\\nbeing sensed, update, and communicate these hypotheses.\\n13The Thousand Brains Project\\nFigure 7: Example of six sensors in Habitat. The view-finder is not connected to any sensor module or learning module\\nand is only used to set up the experiment and for visualization. Each patch connects to one sensor module.\\nFigure 8: Difference between sensors and agents. Agents can move independently of each other while all sensors\\nconnected to one agent move together. An agent itself does not perceive anything without sensors connected to it.\\n14The Thousand Brains Project\\nFigure 9: The 77 objects of the YCB dataset at 0, 0, 0 rotation.\\nImportantly, long-term memory relies on a structured rep-\\nresentation of the world - a reference frame. At various\\ntimes, such as when an object is recognized and being stud-\\nied further, information from the buffer can be processed\\nand integrated into the long-term memory. Finally, each\\nlearning module can also receive and send target states\\nusing a goal-state generator to guide the exploration and\\nmanipulation of the environment. We will discuss goal-\\nstate generators and model-based policies in more detail in\\na later section.\\n9.1 Different Phases of Learning\\nThe learning module is designed to be able to learn objects\\nfrom scratch. This means it is not assumed that we start\\nwith any previous knowledge or even complete objects\\nstored in memory. As such, models in graph memory areupdated after every episode, and learning and inference are\\ntightly intertwined. If an object is recognized, the model\\nof this object is updated with any newly sensed points.\\nIf no object is recognized, a new model is generated and\\nstored in memory. This also means that the whole learning\\nprocedure can be unsupervised, as there are no object labels\\nprovided3.\\nTo keep track of which objects were used for building a\\ngraph (since we do not provide object labels in this unsu-\\npervised learning setup), we store two lists in each learning\\nmodule, mapping between learned graphs and the ground-\\ntruth objects observed in the world. These lists can later\\n3Resetting the buffer at the end of an episode is a weak su-\\npervisory signal if we are changing the object after each episode,\\nhowever this is not always the case, as different episodes often\\nshow the same object from different angles.\\n15The Thousand Brains Project\\nFigure 10: Example Monty system with five sensor modules and learning modules. Each sensor patch perceives a small\\npart of the environment, from which the associated SM extracts features and a pose (location and rotation relative to the\\nbody). This is sent to the LM which models the input and outputs another feature (most likely object ID) and its pose\\n(most likely rotation and location of the object). LMs have lateral connections between one another (dotted lines) to\\ncommunicate possible poses and narrow down their hypotheses faster. In this instance, most of the LMs believe the\\ncurrent object is a red mug, while the middle LM thinks it is most likely sensing a red-blue cylinder object that it has\\npreviously learned about. Through lateral voting the five LMs can quickly narrow down the possible object and pose.\\nbe used for analysis and to determine the performance\\nof the system, but they are not used as a learning signal.\\nThis means learning can happen completely unsupervised\\nwithout any labels being provided.\\nThere are two modes the learning module could be in:\\ntraining andevaluation . They are both very similar as\\nboth use the same procedure of moving and narrowing\\ndown the list of possible objects and poses. The only\\ndifference between the two is that in the training mode,\\nthe models in memory are updated after every episode. In\\npractice, we often learn a series of objects and then save\\nthis Monty instance for a series of downstream evaluations\\nwith learning disabled. This enables controlled evaluations\\nof the model, but it is important to emphasize that the long-\\nterm design of Monty is such that it would constantly be\\nlearning, with no separate learning and evaluation phases.\\nThetraining mode is split into two phases that alternate:\\nThe matching phase and the exploration phase. During the\\nmatching phase the module tries to determine the object\\nID and pose from a series of observations and actions. This\\nis the same as in evaluation. After a terminal condition is\\nmet (object recognized or no match found), the module\\ngoes into the exploration phase . This phase continues to\\ncollect observations and adds them into the buffer the same\\nway as during the previous phase; only the matching step\\nis skipped. The exploration phase is used to add moreinformation to the model representation at the end of an\\nepisode.\\nFor example, the matching might terminate after three\\nsteps, telling us that the past three observations are not con-\\nsistent with any models in memory. This would result in\\nstoring a new model in memory, however a model informed\\nby only three observations is not very useful. Hence, we\\nkeep moving for a specified number of exploratory steps to\\ncollect more information about this object before adding it\\nto memory. This is not necessary during evaluation since\\nwe do not update our models then.\\n9.2 First Generation Learning Modules\\nA learning module (LM) can take a variety of forms, as\\nlong as it models objects with reference frames, and in-\\nterfaces via the Cortical Messaging Protocol (CMP). For\\nexample, an LM might use sparse-distributed representa-\\ntions (SDR) to represent features, and grid-cells for refer-\\nence frames, using similar mechanisms as explored with\\nsynthetic objects in [Lewis et al., 2019]. We have ex-\\nperimented with several variants, but the majority of our\\nwork has so far focused on LMs that leverage explicit, 3D\\ngraphs in Cartesian space. As such, these graph-based\\nLMs can be considered the first-generation of possible im-\\nplementations. You may see occasional references to a\\n‘feature’ or ‘displacement’-based graph-LM, however the\\n16The Thousand Brains Project\\nFigure 11: Processing sensory inputs for the Cortical Messaging Protocol. The sensor patch captures a small area of the\\nobject (blue square), and if the sensor is a camera, it returns an RGBD image. We apply a transform to this image which\\ncalculates the x, y, z locations relative to the agent’s body for each pixel using the depth values and the sensor location.\\nFrom these points in space, the sensor module then calculates the point normal and principal curvature directions at the\\ncenter point of the patch (pose). Additionally, the sensor module can extract pose-independent features such as color\\nand the magnitude of curvature at the center of the patch. The pose (location + point normal and curvature direction)\\nand features make up the observation at time step tand are the output of the sensor module.\\nevidence-based LM is the implementation that we use as\\nthe default for all of our current experiments as it is most\\nrobust to noise and sampling new locations. As such, we\\nwill focus on describing its details. In brief, it makes use\\nof graph-based reference frames where the evidence score\\nassociated with any node in the graph can be iteratively\\nadjusted.\\nWe note that using explicit 3D graphs makes visualization\\nmore intuitive, improves interpretability, and facilitates\\ndebugging. This does not mean that we believe the brain\\nstores explicit graphs with Cartesian coordinates. Future\\ngenerations of LMs might use more neural representations\\nsuch as grid-cells, however we are intentionally abstaining\\nfrom such instantiations until they prove necessary.\\n9.3 The Buffer (Short-Term Memory)\\nEach learning module has a buffer that can be compared\\nto short-term memory. The buffer only stores information\\nfrom the current episode and is reset at the start of every\\nnew episode (and potentially at other events such as mov-\\ning from one object onto another). Its content is used to\\nupdate the graph memory at the end of an episode. The\\nbuffer is also used to retrieve the location observed in the\\nprevious step for calculating displacements.\\n9.4 The Graph Memory (Long-Term Memory)\\nEach learning module has one graph memory which it uses\\nas a long-term memory of previously acquired knowledge.\\nIn the graph learning modules, the memory stores explicit\\nobject models in the form of graphs in 3D Cartesian space.\\nThe graph memory is responsible for storing, updating,\\nand retrieving models from memory.\\n9.5 Object Models\\nObject models are stored in the graph memory and contain\\ninformation about one object. The information they storeis encoded in reference frames and contains poses relative\\nto each other and features at those poses. More specifi-\\ncally, the model encodes an object as a graph with nodes.\\nEach node contains a pose and a list of features. Edge\\ninformation can be used in principle (storing important dis-\\nplacements), but is not currently emphasized. Furthermore,\\ngraphs can generally be arbitrarily large in dimension and\\nmemory, although we are now experimenting with a form\\nof constrained graphs that encourage intelligent use of\\nlimited representational capacity.\\n9.6 Graph Building\\nA graph is constructed from a list of observations (poses,\\nfeatures). Each observation can become a node in the\\ngraph, which in turn connects to its neighbors in the graph\\nby proximity or temporal sequence, indicated by the edges\\nof the graph. Each edge has a displacement associated\\nwith it, which is the action that is required to move from\\none node to the other. Each node can have multiple fea-\\ntures associated with it or simply indicate that there was\\ninformation sensed at that point in space. Each node must\\ncontain location and orientation information in a common,\\nobject-centric reference frame.\\n9.7 Graph Updates\\nIf a graph is not stored in memory yet, the LM will not\\nfind a match during object recognition, and it will add a\\nnew graph to memory.\\nEven if the object is already stored in memory, there may\\nbe new features we can learn about it and incorporate\\ninto the graph. For adding new observations, we need\\nto know the pose of the object relative to our model in\\nmemory. The detected pose is used to rotate and translate\\nnew observations (which are relative to the sensor) into the\\nreference frame of the object.\\nIf a new point is too similar to those already in the graph\\nby some threshold (such as being close in space or having\\n17The Thousand Brains Project\\nFigure 12: First two episodes (separated by a vertical double line) during learning. After we recognize an object\\n(matching phase, blue line) we can explore the object further to collect new information about it (exploration phase,\\npink line). This information can then be added to the model of the object in memory. The top row shows the agent’s\\nmovements during the episodes. The bottom row shows the models in memory. As we are learning from scratch, we\\nhave no model in memory during the first episode.\\nFigure 13: A Graph of features (nodes), linked by displace-\\nments (edges). Each node represents a relative location and\\nstores three pose vectors (for example, the point normal\\nand the two principal curvature directions). Nodes can also\\nhave pose-independent features associated with them, such\\nas color and curvature. The graph stored in memory can\\nthen be used to recognize objects from actual feature-pose\\nobservations.\\nsimilar features), then the LM will not add the point to its\\nlong-term memory. Avoiding the addition of similar points\\nmakes matching more efficient and avoids storing redun-\\ndant information in memory. Instead, the LM stores morepoints where features change quickly (like where the han-\\ndle attaches to the mug) and fewer points where features\\nare not changing as much (such as on a flat surface).\\n9.8 Using Graphs for Prediction and Querying Them\\nWe can use graphs in memory to predict if there will be\\na feature sensed at the next location and what the next\\nsensed feature will be, given an action/displacement (for-\\nward model). This prediction error can then be used for\\ngraph matching to update the possible matches and poses.\\nA graph can also be queried to provide an action that\\nleads from the current feature to a desired feature (inverse\\nmodel). This can be used for a goal-conditioned action pol-\\nicy and more directed exploration. To do this, a hypothesis\\nof the currently sensed object and its pose is required.\\n9.9 The Evidence-Based Learning Module\\nThe evidence-based LM uses a graph representation of\\nobjects, with all of the elements described up until now. In\\naddition, a continuous evidence value is assigned to each\\nhypothesis (which object and pose is being sensed), and\\nthese values are updated with every observation. We use\\nthe movement of a sensor on a colored cylinder, shown in\\nfigure 15, as an example.\\n9.10 Initializing Hypotheses\\nAt the first step of an episode we need to initialize our\\nhypothesis space. This means, we define which objects\\nand poses are possible.\\n18The Thousand Brains Project\\nFigure 14: (top) Building a graph from a buffer of observations. First, similar observations (high spatial proximity\\nand feature similarity) are removed, and then the observations are turned into a graph structure as described above.\\n(bottom-right) An object model that was learned from multiple views and extended over time.\\nAt the beginning of an episode, we consider all objects\\nin an LM’s memory as possible. We also consider any\\nlocation on these objects as possible. We then use the\\nsensed pose features (point normal and principal curvature\\ndirection) to determine the possible rotations of the object.\\nThis is done for each location on the object separately\\nsince we would have different hypotheses of the object\\norientation depending on where we assume we are. For\\nexample, the rotation hypothesis from a point on the top of\\nthe cylinder is 180 degrees different from a hypothesis on\\nthe bottom of the cylinder (see figure 16, top).\\nBy aligning the sensed point normal and curvature direc-\\ntion with the ones stored in the model we usually get two\\npossible rotations for each possible location. We get two\\nsince the curvature direction has a 180-degree ambiguity,\\nmeaning we do not know if it points up or down as we do\\nwith the point normal.\\nFor some locations, we will have more than two possible\\nrotations. This is the case when the first principal curvature\\n(maximum curvature) is the same as the second principal\\ncurvature (minimum curvature), which happens, for exam-\\nple, when we are on a flat surface or a sphere. If this is the\\ncase, the curvature direction is meaningless and we sample\\nN possible rotations along the axis of the point normal.After initializing the hypothesis space we assign an evi-\\ndence count to each hypothesis. Initially, this is 0 but if\\nwe are also observing pose-independent features such as\\ncolor or the magnitude of curvature we can already say that\\nsome hypotheses are more likely than others (see figure 16,\\nbottom).\\nTo calculate the evidence update, we take the difference\\nbetween the sensed features and the stored features in the\\nmodel. At any location in the model where this difference\\nis smaller than the tolerance value set for this feature, we\\nadd evidence to the associated hypotheses proportional\\nto the difference. Generally, we never use features to\\nsubtract evidence, only to add evidence. Therefore, if the\\nfeature difference is larger than the tolerance (like in the\\nblue and flat parts of the cylinder model in figure 16) no\\nadditional evidence is added. The feature difference is\\nalso normalized such that we add a maximum of 1 to the\\nevidence count if we have a perfect match and 0 evidence if\\nthe difference is larger than the set tolerance. A weighting\\nfactor associated with each type of feature can be used to\\nemphasize some more than others.\\nIn the example shown in figure 16 we initialize two pose\\nhypotheses for each location stored in the model, except\\non the top and bottom of the cylinder where we have to\\nsample more because of the undefined curvature directions.\\n19The Thousand Brains Project\\nFigure 15: Sensing a Two-Toned Object A representation of a sensor (camera patch), moving over the surface of a\\ncylinder that is colored red in some parts, and blue in others. We will show how hypotheses are initialized and how\\nevidence is updated based on this example. In this representation, the system collects three observations and performs\\ntwo movements on the colored cylinder. The first observation is on the red rounded part of the cylinder (left), then it\\nmoves up (middle), and finally to the right onto the blue part (right).\\nUsing the sensed features, we update the evidence for each\\nof these pose hypotheses. In locations where both color\\nand curvature match, the evidence is the highest (red). In\\nplaces where only one of those features matches, we have\\na medium-high evidence (yellow) and in areas where none\\nof the features match we add 0 evidence (grey).\\n9.11 Updating Evidence\\nIn all subsequent steps, we are able to use a pose displace-\\nment for updating our hypotheses and their evidence. At\\nthe first step we had not moved yet so we could only use\\nthe sensed features. Now we can look at the difference\\nbetween the current location and the previous location\\nrelative to the body and calculate the displacement.\\nThe relative displacement between two locations can then\\nbe used in the model’s reference frame to test hypotheses.\\nThe displacement is only regarding the location while the\\nrotation of the displacement will still be in the body’s ref-\\nerence frame. To test hypotheses about different object\\nrotations we have to rotate the displacement accordingly.\\nWe take each hypothesis location as a starting point and\\nthen rotated the displacement by the hypothesis rotation.\\nThe endpoint of the rotated displacement is the new pos-\\nsible location for this hypothesis. It basically says \"If I\\nwould have been at location X on the object, the object is\\nin orientation Y , and I move with displacement D, then I\\nwould now be at location Z\". All of these locations and\\nrotations are expressed in the object’s reference frame.\\nEach of these new locations now needs to be checked, and\\nthe information stored in the model at this location needs\\nto be compared to the sensed features. Since the model\\nonly stores discrete points, we often do not have an entry at\\nthe exact search location but look at the nearest neighbors.\\nWe now use both morphology andfeatures to update the\\nevidence. Morphology includes the distance of the search\\nlocation to nearby points in the model and the differencebetween sensed and stored pose features (point normal\\nand curvature direction). If there are no points stored in\\nthe model near the search location then our hypothesis is\\nlikely wrong and we subtract 1 from the evidence count.\\nOtherwise, we calculate the angle between the sensed pose\\nfeatures and the ones stored at the nearby nodes. Depend-\\ning on the magnitude of the angle we can get an evidence\\nupdate between -1 and 1 where 1 is a perfect fit and -1 is\\na 180-degree angle (90 degrees for the curvature direction\\ndue to its symmetry). For the evidence update from the fea-\\ntures, we use the same mechanism as during initialization\\nwhere we calculate the difference between the sensed and\\nstored features. This value can be between 0 and 1 which\\nmeans at any step the evidence update for each hypothesis\\nis in [-1, 2]. A detailed view of the nearest neighbor lookup\\nand feature comparison to determine the evidence update\\nfor one of the hypotheses is shown in Figure 18.\\nThe evidence value from this step is added to the previously\\naccumulated evidence for each hypothesis. At the next\\nstep, the previous steps’ search locations become the new\\nlocation hypotheses. This means that the next displacement\\nstarts where the previous displacement ended given the\\nhypothesis. We note that evidence updates are performed\\nfor all objects in memory. This can be done in parallel\\nsince the updates are independent of each other.\\n9.12 Features and Morphology\\nAs mentioned before, features can only add evidence, not\\nsubtract it. Morphology (location and pose feature match)\\ncan add and subtract evidence. This is because we want\\nto be able to recognize objects even when features are\\ndifferent. For example, if we have a model of a red coffee\\nmug and are presented with a blue one we would still want\\nto recognize a coffee mug.\\nThe idea is that features can add evidence to make recog-\\nnition faster but they cannot reduce the likelihood of a\\n20The Thousand Brains Project\\nFigure 16: Mechanism for initializing hypotheses from one observation. (Top row) Initializing possible poses. For this,\\nwe use all points in the stored model as possible locations and for each location, we calculate how we could rotate the\\nsensed pose features such that they align with the stored pose features. (Bottom row) Initializing evidence counts for all\\nhypotheses. If we do not sense any features, the evidence count is 0 for all possible poses, as signified in grey in the\\ntop right. We can then use sensed features and compare them to stored features at the locations in the model. If the\\ndifference is low, we add evidence proportional to that difference. The colors of dots in the model (bottom left) signify\\nthe stored color at that node. Colors in the hypothesis space (right) signify evidence where grey=0, yellow=medium,\\nand red=high evidence. Note the four points that have high evidence because their stored features matched both the\\ncolor and curvature of the observation.\\nhypothesis. This is only halfway achieved right now, since\\nwe consider relative evidence values and if features add\\nevidence for some hypotheses and not for others, it also\\nmakes them implicitly less likely and can remove them\\nfrom possible matches.\\nA future solution could be to store multiple possible fea-\\ntures or a range of features at the nodes. Alternatively, we\\ncould separate object models more and have one model for\\nmorphology, which can be associated with many feature\\nmaps (kind of like UV maps in computer graphics). This\\nis still an area of active conceptual development.\\n10 LM Outputs and Connectivity\\nA learning module can have three types of output at every\\nstep, all of which adhere to the Cortical Messaging Pro-\\ntocol. The first one is, just like the primary, bottom-up\\ninput, a pose relative to the body and features at that pose.\\nThis could be the most likely object ID (represented as a\\nfeature) and its most likely pose. This output can be sentas input to another learning module or be read out to assess\\nMonty’s performance.\\nThe second output is the LMs vote. If the LM received\\ninput at the current step, it can send out its hypotheses and\\nthe likelihood of them to other LMs that it is connected to.\\nFor more details of how this works in the evidence LM,\\nsee section 10.2.\\nFinally, the LM can also suggest an action in the form of\\nagoal state . This goal state can then either be processed\\nby another learning module and split into sub-goals or by\\nthe motor system and translated into a motor command\\nin the environment. The goal state follows the CMP and\\ntherefore contains a pose relative to the body and features.\\nFor instance, the goal state might indicate a target pose\\nfor the sensor it connects to that would help it recognize\\nthe object faster, or would provide new information about\\nan object for learning. More information on goal-states is\\nprovided in Section 11.\\n21The Thousand Brains Project\\nFigure 17: Updating evidence using two steps. First, we move upwards and sense red and curved features (top row).\\nThen, we move right and sense blue and curved features (bottom row). At each step, we take the hypotheses from\\nthe previous step (first column) and combine them with the sensed displacement (second column). Wherever these\\nhypotheses combined with the displacement end up, defines our search locations (third column). We then look at the\\npoints stored in the model that are near each search location and compare the features stored there with the sensed\\nfeatures to update the evidence for each hypothesis (fourth and fifth column). Colors represent evidence values where\\ndark blue=low, light blue=medium low, grey=0, orange=medium high, and red=high. Arrows in the left column signify\\npose hypotheses. Arrows in the right column signify the hypothesis plus the displacement and the updated evidence for\\nthis hypothesis.\\n10.1 Most Likely Hypothesis and Possible Matches\\nWe use continuous evidence values for our hypotheses, but\\nfor some outputs, statistics, and the terminal condition, we\\nneed to threshold them. This is done using a (currently\\nuser-set) percent-threshold parameter that defines how con-\\nfident we need to be in a hypothesis to make it our final\\nclassification and move on to the next episode.\\nThe threshold is applied in two places: To determine pos-\\nsible matches (the object we are observing) and possible\\nposes (rotation of the object, and location on its surface).\\nIn either case, we look at the maximum evidence value\\nand calculate the parameter-set percent of that value. Any\\nobject or pose that has evidence larger than the maximum\\nevidence minus this relative percentage is considered pos-\\nsible.\\nThe larger the percent threshold is set, the more certain\\nthe model has to be in its hypothesis to reach a terminal\\nstate. This is because the terminal state checks if thereis only one possible hypothesis. If we, for instance, set\\nthe threshold at 20%, there cannot be another hypothesis\\nwith an evidence count above the most likely hypothesis\\nevidence minus 20%.\\nBesides possible matches and possible poses, we also have\\nthe most likely hypothesis. This is simply the maximum ev-\\nidence of all poses, irrespective of any threshold. The most\\nlikely hypothesis within one object defines this object’s\\noverall evidence and the most likely hypothesis overall\\n(the output of the LM) is the maximum evidence value\\nacross all objects and poses. Each LM has a most likely\\nhypothesis at every step, even if it is not confident enough\\nyet to make a classification.\\nFinally, when an object has no hypothesis with a positive\\nevidence count, it is not considered a possible match. If all\\nobjects have only negative evidence, then we do not know\\nthe object we are presented with and the LM creates a new\\nmodel for it in memory.\\n22The Thousand Brains Project\\nFigure 18: Calculating the evidence update for one hypothesis. First, we calculate a search location given the hypothesis\\nand displacement (top left). Then we find the nearest points stored in the model to this search location that are within a\\ngiven radius (top right). For each point in the search radius, we compare the stored features to the sensed features and\\ncalculate the evidence (bottom left). We use the best match to update the hypothesis evidence (bottom right).\\n10.2 Voting with Evidence\\nV oting can help to recognize objects faster as it helps inte-\\ngrate information from multiple matches. In particular, a\\nlearning module is able to recognize objects on its own sim-\\nply through successive movements, however many such\\nmovements may be required. With voting we can perform\\n‘flash inference’ by sharing information between multiple\\nlearning modules. Note that, as shown in figure 21, voting\\nalso works across modalities. This is because votes only\\ncontain information about possible objects and their poses\\nwhich is modality agnostic. At no point does an LM com-\\nmunicate object models or modality specific features to\\nother LMs.\\nAt each step, after an LM has updated its evidence based\\non the current observation, the LM sends out a vote to all\\nits connected LMs. This vote contains its pose hypotheses\\nand the current evidence for each hypothesis. The evidence\\nvalues are scaled to [-1, 1] where -1 is the currently lowest\\nevidence and 1 is the highest. This makes sure that LMs\\nthat received more observations than others do not have an\\noutsized influence. We can also choose to transmit only\\nsome votes by sub-selecting those with a proportionally\\nhigher evidence value, significantly reducing the computa-\\ntional cost of voting.\\nThe votes get transformed using the displacement between\\nthe sensed input poses. We assume that the models inboth LMs were learned at the same time and are therefore\\nin the same reference frame. If this does not hold, the\\nreference transform between the models would also have\\nto be applied here, for example, via a learned association.\\nOnce the votes are in the receiving LM’s reference frame,\\nthe receiving LM updates its evidence values. To do this,\\nit again looks at the nearest neighbor to each hypothesis\\nlocation, but this time the nearest neighbors in the votes.\\nThe distance-weighted average of votes in the search radius\\n(between -1 and 1) is added to the hypothesis evidence.\\n10.3 Terminal Conditions\\nIn our current experimental setup, we divide time into\\nepisodes. Each episode ends when a terminal state is\\nreached. In the object recognition task, this is either no\\nmatch (the model does not know the current object and\\nwe construct a new graph for it), match (we recognized\\nan object as corresponding to a graph in memory), or\\ntime out (we took a maximum number of steps without\\nreaching one of the other terminal states). As a result, any\\ngiven episode contains a variable number of steps, and\\nafter every step, we need to check if a terminal condition\\nwas met. More details on how the terminal condition\\nis determined are available in our online documenta-\\ntion at https://thousandbrainsproject.readme.\\nio/docs/evidence-based-learning-module#\\nterminal-condition .\\n23The Thousand Brains Project\\nFigure 19: Information flow in a graph learning module. At each step the LM receives features and a pose as input.\\nUsing the previous observation stored in the buffer it can calculate a pose displacement. This displacement, together\\nwith the sensed features, is used to evaluate all current hypotheses and update them. In the evidence LM this means\\nupdating their evidence, in the other two LMs it means to remove the hypotheses from the list of hypotheses if incoming\\ninformation is inconsistent with the model’s predictions. Using the current hypotheses and their evidence, the LM can\\nthen output a vote, which is sent to any connected LMs. If there are incoming votes, they will be used for another\\nhypothesis update. Additionally, incoming top-down input can be used to modulate the evidence for different hypotheses.\\nAfter this, the LM outputs its most likely hypothesis (object ID and pose) and a goal state (used for action selection).\\nThe goal state is produced by the goal-state generator, which can use a higher-level goal state and the LMs internal state\\nand models to decide on the best goal state to output. Once matching is completed, the list of features and poses in the\\nbuffer can be used to update the graph memory.\\nNote that an individual LM can reach its terminal state\\nearlier than the overall Monty system. For example, if one\\nLM does not have a model of the shown object, it will\\nquickly reach the no-match state, while the other LMs will\\ncontinue until they recognize the object. The episode only\\nends once a parameter-defined minimum number of LMs\\nhave reached their terminal state.\\n10.4 Connecting LMs into a Heterarchy\\n10.4.1 Why Heterarchy?\\nWe use the term heterarchy to express the notion that infor-\\nmation flow in Monty does not follow a strict hierarchy. In\\naddition to classical hierarchical connections, Monty also\\nhas several non-hierarchical forms of connectivity, anal-\\nogous to long-range connections in the neocortex. Even\\nthough we do speak of lower-level LMs and higher-level\\nLMs at times, this does not mean that information flows in\\na rigid manner from layer 0 to layer N.\\nFirstly, there can exit ‘skip connections’ in the network.\\nA low-level LM or even an SM can directly connect to\\nanother LM which represents far more complex, high-level\\nmodels. As such, it is difficult to clearly identify what\\n\"layer\" an LM belongs to based on the number of previous\\nprocessing steps performed on its input. Instead, LMs can\\nbe grouped into collections based on which LMs vote withone another, which is defined by whether there is (learned)\\noverlap in the objects they model. In other words, voting\\nthrough lateral connections can also occur between LMs\\nthat might classically be viewed as existing at different\\nlevels of a hierarchical system.\\nSecond, there exist several channels of communication\\nin Monty that do not implement a hierarchical passing of\\ninformation (see figure 5). An LM can receive multiple top-\\ndown signals, which again may bypass certain LMs. These\\ntop-down inputs arise from LMs that model compositional\\nobjects, and can provide biasing context. In addition to\\nsupporting inference, top-down inputs can carry goal states,\\nused for decomposing hierarchical action policies.\\nLastly, each LM can send motor outputs directly to the\\nmotor system. This is contrary to the idea that sensory\\ninput is processed through a series of hierarchical steps\\nuntil it reaches a single motor area, which then produces\\nactions. Instead, similar to cortical columns in the brain,\\neach LM in Monty operates as a complete sensorimotor\\nunit. Motor output is not exclusive to the top of a hierarchy\\nbut rather occurs at every level of sensory processing.\\nWhile the term heterarchy is useful to capture this flexi-\\nbility in the connectivity of the architecture, it can also be\\nuseful to use more traditional terms from hierarchy to aid\\nunderstanding. Below we provide further details on the\\nbottom-up and top-down information flow in the system.\\n24The Thousand Brains Project\\nFigure 20: Thresholding evidence values to obtain possible matches and poses. The highest pose evidence within an\\nobject becomes the object’s evidence (top row). The highest evidence over all objects is the LM’s current most likely\\nhypothesis (MLH, indicated by a yellow dot). We then apply the percent threshold to objects and poses separately\\n(bottom row). In this case, objects 0 and 2 are considered possible. Object 1 has an evidence count that is too low\\ncompared to the most likely object. Object 3 has no evidence above 0 and is therefore automatically not possible. We\\napply the same procedure to the poses. Possible poses for objects 1 and 2 are more transparent as, in practice, these are\\nonly calculated for the most likely object.\\n25The Thousand Brains Project\\nFigure 21: V ote sent from LM 1 to LM 2 and used to update the evidence in LM 2. The two LMs receive input from\\ntwo different poses on the mug and two different SMs (left). The possible poses and their evidence in LM 1 are sent\\nto LM 2. In order to put them into the reference frame of LM 2 we need to account for the relative displacement\\nbetween SM 1 and SM 2 and transform the poses accordingly (top right). This transformation is performed in a common\\nreference-frame (e.g. body-centric). The hypotheses of LM 2 are then updated by looking at the nearest neighbor votes\\nof each (bottom right). After voting we have a clear most likely pose in LM 2 that is consistent with the inputs from SM\\n1 and SM 2.\\n10.4.2 Bottom-up connections\\nConnections we refer to as bottom-up are connections\\nfrom SMs to LMs, and connections between LMs that\\ncommunicate an LMs output (the current most likely object\\nID and pose) to the main input channel of another LM (the\\ncurrent sensed feature and pose). The output object ID\\nof the sending LM then becomes a feature in the models\\nlearned in the receiving LM. For example, the sending\\nLM might be modeling a car tire. When the tire model is\\nrecognized, it outputs this and the recognized location and\\norientation of the tire relative to the body. The receiving\\nLM would not get any information about the 3D structure\\nof the tire from the sending LM. It would only receive the\\nobject ID (as a feature) and its pose. This LM could then\\nmodel a car, composed of different parts. Each part, like\\nthe tire, is modeled in detail in a lower-level LM and then\\nbecomes a feature in the higher-level LMs’ model of the\\ncar.\\nThe receiving LM might additionally get input from other\\nLMs and SMs. For example, the LM modeling the car\\ncould also receive direct, low-frequency input from a sen-\\nsor module and incorporate this into its model. This input,however, is usually not as detailed as the input to the LM\\nthat models the tire. In particular, we do not want the\\nhigher-level LM to relearn a detailed model of the entire\\ncar. Instead, we want to learn detailed models of its com-\\nponents and then compose the components into a larger\\nmodel. This way, we can also reuse the model of the tire in\\nother higher-level models, such as for trucks, busses, and\\nwheel barrels.\\n10.4.3 Top-down connections\\nTop-down connections can bias the hypothesis space of\\nthe receiving LM, similar to the influence of votes. For\\nexample, if a higher-level LM recognizes a car, this can\\nbias the lower-level LMs to recognize the components\\nof a car at a particular location and pose. This enables\\nfaster inference and better predictions at lower levels of\\nthe system.\\n11 Action Policies\\nAs noted earlier, policies in thousand-brains systems can\\nbe broken down into those that are model-free and those\\n26The Thousand Brains Project\\nthat are model-based. They enable the system to perform\\nrapid, principled actions, and in the future, will enable\\nit to influence the state of the world. Before we discuss\\nthese however, it is worth discussing the different agents\\nthat can currently be leveraged by Monty. These agents in\\nturn have different action spaces, corresponding to their\\nphysical properties.\\n11.1 Agents and Action Spaces\\nWe currently have two broad types of agents, along with\\ntheir particular action spaces. The first is what we call\\nthedistant agent , which is physically separated from the\\nsurface of the object it is sensing. Like an eye, the action\\nspace is that of a ball-and-socket actuator that can look in\\ndifferent directions.\\nThe second agent is designed to move freely through space,\\nwhile being constrained to follow along the surface of an\\nobject (like a finger). Hence we call it the surface agent . It\\ncan efficiently move around the entire object by following\\nits surface, theoretically reaching any feature on the object.\\nBoth the surface and distant agent can be paired with a\\nmodel-based policy that allows them to make instantaneous\\n\"jumps\" in the absolute coordinates of the environment.\\nThese jumps are currently initiated by model-based poli-\\ncies derived from the LMs, as a form of top-down control.\\nWhile the distant and surface agent were inspired by an\\neye vs. a finger sensing the world, in our simulations they\\nare both connected to the same sensor, an RGBD cam-\\nera. More details on the unique properties that these two\\naction spaces afford are covered in our online documen-\\ntation at https://thousandbrainsproject.readme.\\nio/docs/policy .\\n11.2 Utility Policies\\nBefore an experiment starts, the agent is moved to an appro-\\npriate starting position relative to the object. This serves to\\nset up the conditions desired by the human operator and is\\nanalogous to a neurophysiologist lifting an animal to place\\nit in a particular location and orientation in a lab environ-\\nment. As such, these are considered utility functions or\\n\"policies\", in that they are not driven by the intelligence\\nof Monty, although they currently make use of its internal\\naction spaces. Furthermore, sensory observations that oc-\\ncur during the execution of a utility policy are not sent to\\nthe learning module(s), as they have access to privileged\\ninformation, such as a wider field-of-view camera. Two\\nsuch policies exist, one for the distant agent (\"get good\\nview\"), and one for the surface agent (\"touch object\").\\nFor the former, the distant agent is moved to a \"good view\"\\nsuch that small and large objects in the data set cover\\napproximately a similar space in the camera image (see\\nFigure 22). To determine a good view, we use the view-\\nfinder, which is a camera without zoom that sees a larger\\npicture than the sensor patch. Without this policy, small\\nobjects, such as the dice, may be smaller than the sensorpatch itself, thereby preventing any movement of the sensor\\npatch on the object. For large objects, there is a risk that\\nthe agent is initialized inside the object, as shown in the\\nsecond image in the first row of the figure below.\\nThe \"touch object\" policy serves a similar purpose - the\\nsurface agent is moved sufficiently close such that it is\\nessentially on the surface of the object. This will be im-\\nportant in future work when the surface agent has access\\nto sensory inputs, such as texture, that require maintaining\\nphysical contact with an object.\\n11.3 Model-Free and Model-Based Policies\\nTurning to the core policies of Monty, we can divide these\\ninto two broad categories: model-based policies where ac-\\ntions are determined using the internal, structured models\\nof LMs, and those that only rely on sensory inputs and\\nare therefore model-free. A special case of the model-free\\npolicy is the random policy that is not influenced by any\\nfactors. While simpler, model-free policies can already pro-\\nvide significant benefits to the efficiency of a system, while\\nintroducing minimal computational overhead. On the other\\nhand, model-based policies make use of the learned object\\nmodels and current hypotheses to support deliberate, intel-\\nligent actions. Importantly, model-free and model-based\\npolicies can work in concert for maximal efficiency.\\nModel-free policies make use of sensory input from the\\nSM. These can be compared to the actions that humans\\nperform primarily through sub-cortical structures, such\\nas the motor control required for walking, or balancing\\nwhile riding a bicycle. They can be innate, such as reflexes\\nrelated to potentially dangerous stimuli, or learned.\\nModel-based policies are based on goal-states that are out-\\nput from the goal-state generator of the learning modules.\\nThese goal states follow the CMP and therefore contain\\na pose and features. This information is interpreted as\\na target pose of an object that should be achieved in the\\nworld and is translated into motor commands in the motor\\nsystem.\\n11.3.1 Concrete Model-based and Model-Free\\nPolicies Used in Monty\\nIn the current distant agent, observations are collected by\\nrandom movement of the camera. The only model-free\\ninfluence is that if the sensor patch moves off the object,\\nthe previous action is reversed to make sure we stay on\\nthe object. Policies with random elements also have a\\nmomentum parameter (alpha) that regulates how likely\\nit is to repeat the previous action and allows for more\\ndirectional movement paths.\\nThe surface agent can either use a random walk policy\\n(again with an optional momentum parameter to bias fol-\\nlowing a consistent path), or alternatively make use of the\\n\"curvature-informed\" policy. This policy makes use of\\nsensed principle curvature directions, attempting to fol-\\nlow these where they are present, such as on the rim of a\\n27The Thousand Brains Project\\nFigure 22: Using the same object and agent positions for all objects leads to objects covering different amounts of the\\nsensor view (left). The get_good_view function of the motor system is called once at the beginning of each episode and\\nmakes sure that each object covers a similar amount of space in the view-finder (right).\\nFigure 23: Comparison of different surface-agent policies.\\n(orange) Random movement along the object’s surface.\\n(yellow) model-free policy that follows principal curvature\\ndirections. (green) model-based policy that can jump to\\nspecific locations on the object to actively test and contrast\\nthe current most likely hypotheses.\\ncup, or the handle of a mug. The details of this policy are\\nexpanded upon further below.\\nFinally, both the distant and surface agent can be controlled\\nby a model-based policy, what is called the hypothesis-\\ntesting policy.\\n11.4 Policies for Inference vs. Learning. vs.\\nManipulating the Environment\\nThe policies mentioned above are aimed at efficient infer-\\nence, and constitute the focus of our evaluation environ-\\nments to date. There is also a specialized policy that can be\\nused to ensure sufficient object coverage when the distantagent is learning about new objects, called the scan policy.\\nThis policy starts at the center of the object and moves out-\\nwards on a spiral path. In addition, model-based policies\\ncan also be leveraged to make learning more efficient, such\\nas directing sensory systems to areas of the world that are\\nunder-represented in internal models. As such, this will be\\na subject of future research. Finally, the long-term aim is\\nthat thousand-brains systems not only quickly understand\\ntheir environment, but can also mediate change in the state\\nof the world. As such, action policies that enable the sys-\\ntem to interact with external objects will be a major focus\\nof future work.\\n11.4.1 Policy Algorithm Details\\nDue to their relative complexity, the below sub-sections\\nprovide further detail on the curvature-informed and\\nhypothesis-testing policies.\\nCurvature Informed Policy (Model-Free)\\nThe curvature-informed surface-agent policy is designed\\nto follow the principal curvatures on an object’s surface\\n(where these are present), so as to efficiently explore the\\nrelevant \"parts\" of an object. For example, this policy\\nshould be more likely than a random walk with momentum\\nto efficiently explore components such as the rim of a cup,\\nor the handle of a mug, when these are encountered.\\nTo enable this, the policy is informed by the principal\\ncurvature information available from the sensor-module.\\nIt will alternate between following the minimal cur-\\nvature (e.g. the rim of a cup) and the maximal\\ncurvature (e.g. the curved side of a cylinder). The\\ndecision process that guides the curvature-guided policy\\nis elaborated in our online documentation ( https:\\n//thousandbrainsproject.readme.io/docs/\\npolicy#curvature-informed-policy-details ).\\nWe also go into detail there about the ability of the\\n28The Thousand Brains Project\\nFigure 24: Two samples of the curvature-informed surface policy being used during inference. Blue segments represent\\nmoving randomly with some momentum, white represents following minimal principal curvature, black following\\nmaximal, and green taking an avoidance step. Note in particular how in (b), as the agent moves over the rim of the cup,\\nit realizes it will revisit a previous location, and so takes an avoidance step (green) to bring it in a new direction. Further\\nnote that even when principal curvature might be evident to the human eye, the model may not be receiving a valid\\nPC-input due to noise on the surface of the mesh. Finally, the end point shows the sensed orientation of the current\\nfeature (blue detected point-normal, red and orange detected principal curvatures).\\ncurvature-informed policy to avoid previously sampled\\nlocations, a capability that will likely be implemented as a\\nmodel-based policy in the future.\\nIn Figure 24, two examples of the policy in action are\\nshown.\\nHypothesis-Testing Policy (Model-Based)\\nFor the hypothesis-testing action policy, Monty uses its\\nlearned internal models of objects. In particular, an LM’s\\nlearned models enable hypotheses about the current ID\\nand pose of the object that it is perceiving. By comparing\\nthe models of the most likely hypotheses, the hypothesis-\\ntesting policy enables an LM to propose a point in space\\nto move to that can rapidly disambiguate the actual object\\nobserved.\\nTo determine the most distinguishing part of the object\\nto test, a form of \"graph mismatch\" is employed. This\\ntechnique takes the most likely and second most likely\\nobject graphs, and using their most likely poses, overlays\\nthem in an internal (\"mental\") space. It then determines for\\nevery point in the most likely graph, how far the nearest\\nneighbor is in the second graph. The output is the point in\\nthe first graph that has the most distant nearest neighbor\\n(Figure 25).\\nAs a model-based policy, the LM communicates the de-\\nsired action in the form of a goal-state. This goal-state rep-\\nresents a state that the motor-system should achieve, such\\nas sensing the point of interest with a sensor-augmented\\nactuator.\\nBringing this together, consider the example of recogniz-\\ning a mug, where the LM also knows about other cylin-drical objects such as cans. If the most likely object was\\na mug, and the second most likely object a can of soup,\\nthen a point on the handle of the mug would have the most-\\ndistant nearest neighbor to the can-of-soup graph. The LM\\nwould communicate to the motor systems a goal-state to\\nbe observing the point in body-centric coordinates corre-\\nsponding to the handle of the mug. In this way, the LM\\ncan coordinate intelligent behavior in the motor-system,\\nwithout the motor-system needing to know anything about\\nobjects like mugs or soup cans.\\nFor the graph-mismatch component, using the Euclidean\\ndistance between graph points is a reasonable heuristic\\nfor identifying potentially diagnostic differences in the\\nstructures of two objects. However, in the future we may\\nalso want to look at distances in feature space, which\\nwill be particularly interesting once features represent sub-\\ncomponents in higher-level LMs.\\nIn addition to being able to compare the top two most likely\\nobjects in such a way, one can just focus on the most likely\\nobject, and use this same technique to compare the two\\nmost likely poses of the same object.\\nDetails for the decision-process that determines exactly\\nwhen a hypothesis-testing action is generated by an LM\\nare provided in our online documentation at https:\\n//thousandbrainsproject.readme.io/docs/\\npolicy#hypothesis-driven-policy-details .\\n29The Thousand Brains Project\\nFigure 25: The hypothesis-testing policy. The left and right plots show where the agent is in the external world at two\\ndifferent time-steps (surface agent represented with ball-and-pole, where the pole points in the direction the agent is\\nfacing). The central plot shows the LMs internal model of the hypothesis spaces. The LM uses its estimate of its current\\nlocation and pose on the two most-likely objects (spoon and knife) to compare their relative orientations in its internal\\n\"mental\" space. Once overlaid, the graph-mismatch technique proposes testing a part of the head of the spoon (red-spot,\\ncenter) as it maximally distinguishes that graph from the other. Here Euclidean distance in 3D space is used, but this\\nprocess can also be performed in feature space. In the final panel, we see the agent after instantaneously moving to the\\ntest point.\\n11.5 Long Term Policy View\\n11.5.1 Learning, and the Interplay of Model-Free\\nand Model-Based Policies\\nIt is worth emphasizing that future versions of Monty will\\ncertainly leverage various advanced learning techniques,\\nincluding reinforcement learning. It is important to note\\ntherefore that our intent is not to explicitly implement\\npolicies for every conceivable scenario an agent might\\nface. Rather, the policies we are developing are intended\\nas a reasonable set of primitives that other policies might\\nmake use of, without having to learn them from the ground\\nup. This is analogous to examples such as i) the tendency\\nof infants to attend to human faces (innate, model-free,\\nref) ii) the structural knowledge of bicycles and people\\nthat enables a child to observe an adult bicycling, and\\nthereby efficiently attempt peddling (learned, model-based)\\niii) the fine-motor coordination and balance that enables\\nproficiently riding a bicycle (learned, model-free).\\n11.5.2 Abstract Spaces\\nIt is also worth noting that the policy primitives we are\\ndeveloping will likely prove useful as we move from 3D\\nphysical space to more abstract spaces. For example, in\\nsuch settings, we might still want to have an inductive\\nbias that moves along dimensions of maximal or minimalvariation of a manifold, akin to the curvature-guided sur-\\nface policy. Similarly, the mental alignment of structured\\nrepresentations to identify the most relevant points that\\ndistinguish different concepts would be useful in a vari-\\nety of abstract settings, such as comparing computational\\nalgorithms.\\n11.6 Hierarchical Model-Based Policies\\nFinally, the model-based policy we have so far described\\nmakes use of only a single level of reference-frame based\\nrepresentations. We are in the process of implementing hi-\\nerarchical, compositional representations, which will also\\nenable hierarchical, model-based policies. In particular,\\nthe goal-states that an LM outputs can also be received by\\nother LMs. This affords the ability to decompose a com-\\nplex task into simpler tasks, where an LM is responsible\\nfor a given task as a function of what it knows about the\\nworld. For example, at the highest level of a system there\\nmight be an innate requirement, such as a person wishing\\nto improve their energy levels. An LM that models the way\\nalertness can be increased would have a goal of making a\\ncup of coffee. This goal-state could be sent to an LM that\\nmodels a kitchen environment, which will in turn recruit\\nan LM that models the structure and behavior of coffee\\nmachines. Eventually, such goal-states will always decom-\\npose into a simple goal-state that can be sent directly to\\nmotor-systems. The signal to the motor-system is analo-\\n30The Thousand Brains Project\\ngous to the direct, motor projections found in all cortical\\ncolumns of the cortex, while hierarchical goal-states likely\\ncorrespond to the connectivity between cortical columns.\\nSome novel actions may require complex, model-based\\nplanning of a system’s own actuators, such as trying to\\npush a computer mouse with your elbow, or holding a pen\\nbetween the knuckles of your left and right ring-fingers.\\nThis will likely require LMs that model actuator objects\\n(e.g. robotic limbs), rather than objects external to the\\nsystem, analogous to the motor cortex of the brain. Such\\nsystems might receive goal-states from other regions in\\nthe system, before ultimately recruiting simpler, model-\\nfree policies present at the level of the motor system. After\\nmany repetitions, policies that were originally model-based\\nand relatively laborious can become model free.\\n12 Conclusion\\nWe have outlined our vision for sensorimotor AI based on\\nthe operating principles of the neocortex. Thousand-brains\\nsystems replicate a core computational-unit - the cortical\\ncolumn implemented as the learning-module - to model ob-\\njects in the world in any sensory modality, and at any level\\nof abstraction. Each unit operates as a semi-independent\\nsensorimotor system, but is also able to communicate key\\ninformation with other learning modules via a common\\ncommunication protocol. By leveraging structured, inter-\\nnal models, learning-modules can rapidly learn about the\\nworld, and leverage this knowledge for sophisticated poli-\\ncies. Building on these core concepts, we described Monty,\\nthe first instantiation of a thousand-brains system. While\\na simple, first-generation implementation, we believe that\\nthese same principles will apply to all future thousand-\\nbrains systems, affording increasing intelligence as the so-\\nphistication of their implementations grows. More broadly,\\nwe hold that the principles described here will be the foun-\\ndation for a new type of AI, one that efficiently learns\\ngeneralizable representations from sensorimotor data.\\n13 Acknowledgments\\nWe would like to thank the following individuals for invalu-\\nable discussions on the Thousand Brains theory and Monty\\nconcepts: Subutai Ahmad, Heiko Hoffmann, and Kevin\\nHunter. In addition to such contributions to discussions,\\nwe would like to thank the following individuals for their\\ncontributions to the Monty code-base: Ben Cohen, Jad\\nHanna, Abhiram Iyer, Ramy Mounir, Luiz Scheinkman,\\nPhilip Shamash, and Lucas Souza.\\nReferences\\nJeff Hawkins, Marcus Lewis, Mirko Klukas, Scott Purdy,\\nand Subutai Ahmad. A framework for intelligence and\\ncortical function based on grid cells in the neocortex.\\nFrontiers in Neural Circuits , 2019. ISSN 16625110.\\ndoi:10.3389/fncir.2018.00121.Vernon B. Mountcastle. The columnar organization of\\nthe neocortex. Brain , 120(4), 1997. ISSN 00068950.\\ndoi:10.1093/brain/120.4.701.\\nG Edelman and V Mountcastle. The mindful brain: Cor-\\ntical organization and the group-selective theory of\\nhigher brain function. pp, 100, 1978. URL https:\\n//psycnet.apa.org/record/1979-25355-000 .\\nDanny Driess, Fei Xia, Mehdi S.M. Sajjadi, Corey Lynch,\\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\\nworth, Sergey Levine, Vincent Vanhoucke, Karol Haus-\\nman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor\\nMordatch, and Pete Florence. Palm-e: An embodied\\nmultimodal language model. In Proceedings of Machine\\nLearning Research , volume 202, 2023.\\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agar-\\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\\ning Bao, Mohammad Bavarian, Jeff Belgum, Irwan\\nBello, Jake Berdine, Gabriel Bernadett-Shapiro, Christo-\\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim\\nBrooks, Miles Brundage, Kevin Button, Trevor Cai,\\nRosie Campbell, Andrew Cann, Brittany Carey, Chelsea\\nCarlson, Rory Carmichael, Brooke Chan, Che Chang,\\nFotis Chantzis, Derek Chen, Sully Chen, Ruby Chen,\\nJason Chen, Mark Chen, Ben Chess, Chester Cho, Casey\\nChu, Hyung Won Chung, Dave Cummings, Jeremiah\\nCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry,\\nNoah Deutsch, Damien Deville, Arka Dhar, David Do-\\nhan, Steve Dowling, Sheila Dunning, Adrien Ecoffet,\\nAtty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,\\nNiko Felix, Simón Posada Fishman, Juston Forte, Is-\\nabella Fulford, Leo Gao, Elie Georges, Christian Gib-\\nson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha\\nGontijo-Lopes, Jonathan Gordon, Morgan Grafstein,\\nScott Gray, Ryan Greene, Joshua Gross, Shixiang Shane\\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu,\\nJoost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang,\\nAngela Jiang, Roger Jiang, Haozhun Jin, Denny Jin,\\nShino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,\\nŁukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Ni-\\ntish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,\\nJong Wook Kim, Christina Kim, Yongjik Kim, Jan Hen-\\ndrik Kirchner, Jamie Kiros, Matt Knight, Daniel Koko-\\ntajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-\\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo,\\nMichael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade\\nLeung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly\\nLin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,\\nRyan Lowe, Patricia Lue, Anna Makanju, Kim Mal-\\nfacini, Sam Manning, Todor Markov, Yaniv Markovski,\\n31The Thousand Brains Project\\nBianca Martin, Katie Mayer, Andrew Mayne, Bob Mc-\\nGrew, Scott Mayer McKinney, Christine McLeavey,\\nPaul McMillan, Jake McNeil, David Medina, Aalok\\nMehta, Jacob Menick, Luke Metz, Andrey Mishchenko,\\nPamela Mishkin, Vinnie Monaco, Evan Morikawa,\\nDaniel Mossing, Tong Mu, Mira Murati, Oleg Murk,\\nDavid Mély, Ashvin Nair, Reiichiro Nakano, Rajeev\\nNayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki,\\nAlex Paino, Joe Palermo, Ashley Pantuliano, Giambat-\\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perelman,\\nFilipe de Avila Belbute Peres, Michael Petrov, Henrique\\nPonde de Oliveira Pinto, Michael, Pokorny, Michelle\\nPokrass, Vitchyr H Pong, Tolly Powell, Alethea Power,\\nBoris Power, Elizabeth Proehl, Raul Puri, Alec Rad-\\nford, Jack Rae, Aditya Ramesh, Cameron Raymond,\\nFrancis Real, Kendra Rimbach, Carl Ross, Bob Rot-\\nsted, Henri Roussez, Nick Ryder, Mario Saltarelli,\\nTed Sanders, Shibani Santurkar, Girish Sastry, Heather\\nSchmidt, David Schnurr, John Schulman, Daniel Selsam,\\nKyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Mad-\\ndie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl,\\nBenjamin Sokolowsky, Yang Song, Natalie Staudacher,\\nFelipe Petroski Such, Natalie Summers, Ilya Sutskever,\\nJie Tang, Nikolas Tezak, Madeleine B Thompson, Phil\\nTillet, Amin Tootoonchian, Elizabeth Tseng, Preston\\nTuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea\\nV oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,\\nBen Wang, Jonathan Ward, Jason Wei, C J Weinmann,\\nAkila Welihinda, Peter Welinder, Jiayi Weng, Lilian\\nWeng, Matt Wiethoff, Dave Willner, Clemens Win-\\nter, Samuel Wolrich, Hannah Wong, Lauren Work-\\nman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Woj-\\nciech Zaremba, Rowan Zellers, Chong Zhang, Mar-\\nvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang\\nZhuang, William Zhuk, and Barret Zoph. GPT-4\\ntechnical report. arXiv [cs.CL] , March 2023. URL\\nhttp://arxiv.org/abs/2303.08774 .\\nKevin Black, Noah Brown, Danny Driess, Adnan Es-\\nmail, Michael Equi, Chelsea Finn, Niccolo Fusai,\\nLachy Groom, Karol Hausman, Brian Ichter, Szymon\\nJakubczak, Tim Jones, Liyiming Ke, Sergey Levine,\\nAdrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl\\nPertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong,\\nAnna Walling, Haohuan Wang, and Ury Zhilinsky. pi0:\\nA vision-language-action flow model for general robot\\ncontrol. arXiv , 10 2024. URL http://arxiv.org/\\nabs/2410.24164 .\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias\\nSpringenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ash-\\nley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.A generalist agent. Transactions on Machine Learn-\\ning Research , 5 2022. URL http://arxiv.org/abs/\\n2205.06175 .\\nSIMA Team, Maria Abi Raad, Arun Ahuja, Catarina\\nBarros, Frederic Besse, Andrew Bolt, Adrian Bolton,\\nBethanie Brownfield, Gavin Buttimore, Max Cant,\\nSarah Chakera, Stephanie C. Y . Chan, Jeff Clune,\\nAdrian Collister, Vikki Copeman, Alex Cullum, Ishita\\nDasgupta, Dario de Cesare, Julia Di Trapani, Yani\\nDonchev, Emma Dunleavy, Martin Engelcke, Ryan\\nFaulkner, Frankie Garcia, Charles Gbadamosi, Zhi-\\ntao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gre-\\ngor, Arne Olav Hallingstad, Tim Harley, Sam Haves,\\nFelix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson,\\nSteph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic,\\nLaura Kampis, Rosemary Ke, Thomas Keck, Junkyung\\nKim, Oscar Knagg, Kavya Kopparapu, Rory Lawton,\\nAndrew Lampinen, Shane Legg, Alexander Lerchner,\\nMarjorie Limont, Yulan Liu, Maria Loks-Thompson,\\nJoseph Marino, Kathryn Martin Cussons, Loic Matthey,\\nSiobhan Mcloughlin, Piermaria Mendolicchio, Hamza\\nMerzic, Anna Mitenkova, Alexandre Moufarek, Vale-\\nria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke\\nPan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David\\nReichert, John Reid, Pierre Harvey Richemond, Tyson\\nRoberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha\\nSandars, Daniel P. Sawyer, Tim Scholtes, Guy Sim-\\nmons, Daniel Slater, Hubert Soyer, Heiko Strathmann,\\nPeter Stys, Allison C. Tam, Denis Teplyashin, Tay-\\nfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus\\nWainwright, Jane X. Wang, Zhengdong Wang, Daan\\nWierstra, Duncan Williams, Nathaniel Wong, Sarah\\nYork, and Nick Young. Scaling instructable agents\\nacross many simulated worlds. arXiv , 3 2024. URL\\nhttp://arxiv.org/abs/2404.10179 .\\nJeff Hawkins and Subutai Ahmad. Why Neurons Have\\nThousands of Synapses, a Theory of Sequence Memory\\nin Neocortex. Frontiers in Neural Circuits , 10, 2016.\\nISSN 16625110. doi:10.3389/fncir.2016.00023.\\nJeff Hawkins, Subutai Ahmad, and Yuwei Cui. A\\nTheory of How Columns in the Neocortex Enable\\nLearning the Structure of the World. Frontiers in\\nNeural Circuits , 11(October):1–18, 2017. ISSN\\n1662-5110. doi:10.3389/fncir.2017.00081. URL\\nhttp://journal.frontiersin.org/article/10.\\n3389/fncir.2017.00081/full .\\nSubutai Ahmad and Luiz Scheinkman. How Can We Be\\nSo Dense? The Robustness of Highly Sparse Repre-\\nsentations. ICML 2019 Workshop on Uncertainty and\\nRobustness in Deep Learning , 2019.\\nMarcus Lewis, Scott Purdy, Subutai Ahmad, and Jeff\\nHawkins. Locations in the neocortex: A theory of sen-\\nsorimotor object recognition using cortical grid cells.\\nFrontiers in Neural Circuits , 2019. ISSN 16625110.\\ndoi:10.3389/fncir.2019.00022.\\nJudy A. Prasad, Briana J. Carroll, and S. Murray Sher-\\nman. Layer 5 Corticofugal Projections from diverse\\n32The Thousand Brains Project\\ncortical areas: Variations on a pattern of thalamic and\\nextrathalamic targets. Journal of Neuroscience , 40(30),\\n2020. ISSN 15292401. doi:10.1523/JNEUROSCI.0529-\\n20.2020.\\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets,\\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub,\\nJia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh,\\nand Dhruv Batra. Habitat: A Platform for Embodied\\nAI Research. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , 2019.\\nAndrew Szot, Alex Clegg, Eric Undersander, Erik Wij-\\nmans, Yili Zhao, John Turner, Noah Maestre, Mustafa\\nMukadam, Devendra Chaplot, Oleksandr Maksymets,\\nAaron Gokaslan, Vladimir V ondrus, Sameer Dharur,\\nFranziska Meier, Wojciech Galuba, Angel Chang, Zsolt\\nKira, Vladlen Koltun, Jitendra Malik, Manolis Savva,\\nand Dhruv Batra. Habitat 2.0: Training home assis-\\ntants to rearrange their habitat. In Advances in Neural\\nInformation Processing Systems (NeurIPS) , 2021.\\nXavi Puig, Eric Undersander, Andrew Szot, Mikael Dal-\\nlaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai,\\nAlexander William Clegg, Michal Hlavac, Tiffany Min,\\nTheo Gervet, Vladimir V ondrus, Vincent-Pierre Berges,\\nJohn Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal\\nKalakrishnan, Jitendra Malik, Devendra Singh Chaplot,\\nUnnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh\\nMottaghi. Habitat 3.0: A co-habitat for humans, avatars\\nand robots, 2023.\\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srini-\\nvasa, Pieter Abbeel, and Aaron M. Dollar. The ycb\\nobject and model set: Towards common benchmarks\\nfor manipulation research. In 2015 International Con-\\nference on Advanced Robotics (ICAR) , pages 510–517,\\n2015. doi:10.1109/ICAR.2015.7251504.\\n33'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZbXCUnsaAjL"
      },
      "source": [
        "Initialize connection to DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dX3dRf9PaCdE"
      },
      "outputs": [],
      "source": [
        "cassio.init(\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    database_id=ASTRA_DB_ID,\n",
        "    # secure_connect_bundle=\"secure-connect-astradb.zip\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aek_iNq-aakQ"
      },
      "source": [
        "Create the langchain embedding and llm objects for later usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "0m-9dT33aX4H"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Vbg7rNCva6qU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# to split the text using thr character text split such that it should not increase token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "texts=text_splitter.split_text(raw_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRsWfWnbf38_",
        "outputId": "1a1c78af-d9a3-4fcd-f83e-b652cb8afa8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['THETHOUSAND BRAINS PROJECT : A N EWPARADIGM FOR\\nSENSORIMOTOR INTELLIGENCE\\nViviane Clay∗\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\nvclay@thousandbrains.org\\nNiels Leadholm*\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\nnleadholm@thousandbrains.orgJeff Hawkins\\nNumenta, Inc.\\nRedwood City\\nCA, United States\\njhawkins@thousandbrains.org\\nDecember 25, 2024\\nABSTRACT\\nArtificial intelligence has advanced rapidly in the last decade, driven primarily by progress in the\\nscale of deep-learning systems. Despite these advances, the creation of intelligent systems that\\ncan operate effectively in diverse, real-world environments remains a significant challenge. In\\nthis white paper, we outline the Thousand Brains Project, an ongoing research effort to develop\\nan alternative, complementary form of AI, derived from the operating principles of the neocortex.\\nWe present an early version of a thousand-brains system, a sensorimotor agent that is uniquely',\n",
              " 'an alternative, complementary form of AI, derived from the operating principles of the neocortex.\\nWe present an early version of a thousand-brains system, a sensorimotor agent that is uniquely\\nsuited to quickly learn a wide range of tasks and eventually implement any capabilities the human\\nneocortex has. Core to its design is the use of a repeating computational unit, the ‘learning module’,\\nmodeled on the cortical columns found in mammalian brains. Each learning module operates as\\na semi-independent unit that can model entire objects, represents information through spatially\\nstructured reference frames, and both estimates and is able to effect movement in the world. Learning\\nis a quick, associative process, similar to Hebbian learning in the brain, and leverages inductive\\nbiases around the spatial structure of the world to enable rapid and continual learning. Multiple\\nlearning modules can interact with one another both hierarchically and non-hierarchically via a',\n",
              " 'biases around the spatial structure of the world to enable rapid and continual learning. Multiple\\nlearning modules can interact with one another both hierarchically and non-hierarchically via a\\ncortical messaging protocol (CMP), creating more abstract representations and supporting multimodal\\nintegration. We outline the key principles motivating the design of thousand-brains systems and\\nprovide details about the implementation of Monty, our first instantiation of such a system. Code\\ncan be found at https://github.com/thousandbrainsproject/tbp.monty , along with more\\ndetailed documentation at https://thousandbrainsproject.readme.io/ .\\nKeywords Sensorimotor ·Neocortex ·Embodied ·General Intelligence ·Reference Frames ·Spatial Representations ·\\nModel-Based ·World Models ·Canonical Microcircuit\\n∗Joint first authors.arXiv:2412.18354v1  [cs.AI]  24 Dec 2024The Thousand Brains Project\\n1 Introduction\\nWe are developing a platform for building AI and robotics',\n",
              " '∗Joint first authors.arXiv:2412.18354v1  [cs.AI]  24 Dec 2024The Thousand Brains Project\\n1 Introduction\\nWe are developing a platform for building AI and robotics\\napplications using the same principles as the human brain,\\na broad research initiative called the Thousand Brains\\nProject. The principles this project builds on are fundamen-\\ntally different from those used in deep learning, currently\\nthe most prevalent form of AI. Therefore, our platform\\nrepresents an alternative form of AI, one that we believe\\nwill play an ever-increasing role in the future.\\nThis paper outlines the motivation of the Thousand Brains\\nProject, as well as the technical details of the underlying\\nalgorithm for sensorimotor intelligence. The aim is to\\nenable developers to build AI applications that are more\\nintelligent, more flexible, and more capable in the many\\napplications that deep learning methods fail. Core to the\\ndesign of thousand-brains systems are the principles laid',\n",
              " 'intelligent, more flexible, and more capable in the many\\napplications that deep learning methods fail. Core to the\\ndesign of thousand-brains systems are the principles laid\\nout in the Thousand Brains Theory [Hawkins et al., 2019],\\na theory of intelligence derived from neuroscientific evi-\\ndence of the anatomy and function of the neocortex. One\\ncore principle of the theory builds on the work of Ver-\\nnon Mountcastle, who argued that the power of the mam-\\nmalian brain lies in its re-use of cortical columns as the\\nprimary computational unit [Mountcastle, 1997, Edelman\\nand Mountcastle, 1978]. In honor of Mountcastle’s idea,\\nwe name the first practical implementation of a thousand\\nbrains system \"Monty\". The code for building and exper-\\nimenting with Monty can be found at https://github.\\ncom/thousandbrainsproject/tbp.monty .\\nOne key differentiator between thousand-brains systems\\nand other AI technologies is that the former are built with',\n",
              " 'com/thousandbrainsproject/tbp.monty .\\nOne key differentiator between thousand-brains systems\\nand other AI technologies is that the former are built with\\nembodied, sensorimotor learning at their core. Sensorimo-\\ntor systems learn by sensing different parts of the world\\nover time while interacting with it. For example, as you\\nmove your body, your limbs, and your eyes, the input to\\nyour brain changes. In thousand-brains systems, the learn-\\ning derived from continuous interaction with an environ-\\nment represents the foundational knowledge that supports\\nall other functions. This contrasts with the growing ap-\\nproach that sensorimotor interactions are a sub-problem\\nthat can be solved by beginning with an architecture trained\\non a mixture of internet-scale language and multi-media\\ndata [Driess et al., 2023, OpenAI et al., 2023, Black et al.,\\n2024]. In addition to sensorimotor interaction being the\\ncore basis for learning, the centrality of sensorimotor learn-',\n",
              " 'data [Driess et al., 2023, OpenAI et al., 2023, Black et al.,\\n2024]. In addition to sensorimotor interaction being the\\ncore basis for learning, the centrality of sensorimotor learn-\\ning manifests in the design choice that all levels of pro-\\ncessing are sensorimotor. As will become clear, sensory\\nand motor processing are not broken up and handled by\\ndistinct architectures, or limited to a single, global action\\noutput [Reed et al., 2022, Driess et al., 2023, Team et al.,\\n2024, Black et al., 2024]. Instead, sensation and motor\\noutputs play a crucial role at every point in thousand-brains\\nsystems where information is processed.\\nA second differentiator is that our sensorimotor systems\\nlearn structured models, using reference frames , explicit\\ncoordinate systems within which locations and rotations\\ncan be represented. Internal models derived from these ref-erence frames keep track of where their sensors are relative\\nto things in the world. Models are learned by assigning',\n",
              " 'can be represented. Internal models derived from these ref-erence frames keep track of where their sensors are relative\\nto things in the world. Models are learned by assigning\\nsensory observations to locations in reference frames. In\\nthis way, the models learned by sensorimotor systems are\\nstructured, similar to CAD models in a computer. This\\nallows the system to quickly learn the structure of the\\nworld and how to manipulate objects to achieve a variety\\nof goals, what is sometimes referred to as a world model .\\nAs with sensorimotor learning, reference frames are used\\nthroughout all levels of information processing, includ-\\ning the representations of not only environments but also\\nphysical objects and abstract concepts - even the simplest\\nrepresentations are represented within a reference frame.\\nThere are numerous advantages to sensorimotor learning\\nand reference frames. At a high level, you can think about\\nall the ways humans are different from today’s AI. We',\n",
              " 'There are numerous advantages to sensorimotor learning\\nand reference frames. At a high level, you can think about\\nall the ways humans are different from today’s AI. We\\nlearn quickly and continuously, constantly updating our\\nknowledge of the world as we go about our day. We do not\\nhave to undergo a lengthy and expensive training phase\\nto learn something new. We interact with the world and\\nmanipulate tools and objects in sophisticated ways that\\nleverage our knowledge of how things are structured. For\\nexample, we can explore a new app on our phone and\\nquickly figure out what it does and how it works based on\\nother apps we know. We actively test hypotheses to fill in\\nthe gaps in our knowledge. We also learn from multiple\\nmodalities and these different sensory inputs work together\\nseamlessly. For example, we may learn what a new tool\\nlooks like with a few glances and then immediately know\\nhow to grab and interact with the object via touch. Finally,',\n",
              " 'seamlessly. For example, we may learn what a new tool\\nlooks like with a few glances and then immediately know\\nhow to grab and interact with the object via touch. Finally,\\nwe carry out complex, planned actions that leverage our\\nknowledge of the world to enable intelligent behavior in\\nnew settings.\\nOne of the most important discoveries about the brain\\nis that most of what we think of as intelligence, from\\nseeing, to touching, to hearing, to conceptual thinking,\\nto language, is enabled by a common neural algorithm\\n[Mountcastle, 1997]. All aspects of intelligence are created\\nby the same sensorimotor mechanism. In the neocortex,\\nthis mechanism is implemented in each of the thousands of\\ncortical columns. This means we can create many different\\ntypes of intelligent systems using a set of common building\\nblocks. The architecture we are creating is built on this\\npremise. Thousand-brains systems will provide the core\\ncomponents and developers will then be able to assemble',\n",
              " 'blocks. The architecture we are creating is built on this\\npremise. Thousand-brains systems will provide the core\\ncomponents and developers will then be able to assemble\\nwidely varying AI and robotics applications using them in\\ndifferent numbers and arrangements. We now elaborate on\\nthe high-level motivations of the Thousands Brains Project\\n(TBP), before describing the technical details of Monty,\\nthe first instance of a thousand-brains system.\\n2 The Thousand Brains Project\\n2.1 Long Term Goals\\nA central long-term goal is to build a universal platform and\\nmessaging protocol for intelligent sensorimotor systems.\\nWe call this protocol the \"Cortical Messaging Protocol\"\\n2The Thousand Brains Project\\n(CMP). The CMP can be used as an interface between dif-\\nferent modules , and its universality is central to the ease of\\nuse of the SDK we are developing. For instance, one per-\\nson may have modules optimized for flying a drone using\\nbirds-eye observations, while another may be working with',\n",
              " 'use of the SDK we are developing. For instance, one per-\\nson may have modules optimized for flying a drone using\\nbirds-eye observations, while another may be working with\\ndifferent sensors and actuators regulating a smart home.\\nDrone operation and smart-home control are quite different\\nsettings, but the modules used in these settings should be\\nable to communicate through the same channels defined\\nhere. Furthermore, a setup for a larger home with multi-\\nple drones might require more modules to fully learn and\\ncontrol the system. The CMP is designed to enable rapid\\nscaling of thousand-brains systems as required. Finally,\\nthird parties could develop sensor modules and learning\\nmodules (terms which we will shortly define) according\\nto their specific requirements, but they would be compati-\\nble with all existing modules due to the shared messaging\\nprotocol.\\nA second goal of the TBP is to be a catalyst for a whole\\nnew way of thinking about machine intelligence. The',\n",
              " 'ble with all existing modules due to the shared messaging\\nprotocol.\\nA second goal of the TBP is to be a catalyst for a whole\\nnew way of thinking about machine intelligence. The\\nprinciples of the TBP differ from many principles of pop-\\nular AI methodologies today and are more in line with\\nthe principles of learning in the brain. Most concepts pre-\\nsented here derive from the Thousand Brains Theory (TBT)\\n[Hawkins et al., 2019] and experimental evidence about\\nhow the brain works. Modules in thousand-brains systems\\nare inspired by cortical columns in the neocortex [Edelman\\nand Mountcastle, 1978, Mountcastle, 1997]. The CMP\\nbetween modules relies on object ID and pose information,\\nas could be encoded in neural activity. The communica-\\ntion process is analogous to long-range connections in the\\nneocortex.\\nIn our implementation, we do not need to strictly adhere\\nto all biological details, and it is important to note that\\nshould an engineering solution serve us better for imple-',\n",
              " 'neocortex.\\nIn our implementation, we do not need to strictly adhere\\nto all biological details, and it is important to note that\\nshould an engineering solution serve us better for imple-\\nmenting certain aspects, then it is acceptable to deviate\\nfrom the neuroscience. For instance, we do not need to\\nsimulate spikes and can implement the general algorithm\\nto be efficient on today’s hardware. In general, the inner\\nworkings of the modules can can vary in implementation\\ndetail and do not have to rely on neuroscience as long\\nas they adhere to the CMP. However, the core principles\\nof the TBP are motivated by what we have learned from\\nstudying the neocortex. Furthermore, we expect that in\\nmany instances, the TBP will bring together prior work into\\na single framework, including sparsity, active dendrites,\\nsequence memory, and grid cells [Hawkins and Ahmad,\\n2016, Hawkins et al., 2017, Ahmad and Scheinkman, 2019,\\nHawkins et al., 2019, Lewis et al., 2019].',\n",
              " 'sequence memory, and grid cells [Hawkins and Ahmad,\\n2016, Hawkins et al., 2017, Ahmad and Scheinkman, 2019,\\nHawkins et al., 2019, Lewis et al., 2019].\\nFinally, it will be important to showcase the capabilities of\\nour approach. We will work towards creating non-trivial\\ndemos where the implementation showcases capabilities\\nthat would be hard to demonstrate any other way. This may\\nnot be one specific task but could play to the strength of this\\nsystem to tackle a wide variety of tasks. We will also work\\non making Monty an easy-to-use open-source SDK that\\nother practitioners can apply and test on their applications.We want this to be a platform for all kinds of sensorimotor\\napplications and not just a specific technology showcase.\\n2.2 Core Principles\\nWe have a set of guiding principles that steer the Thousand\\nBrains Project. Throughout the life of the project, there\\nmay be several different implementations, and within each\\nimplementation, there may be different versions of the core',\n",
              " 'Brains Project. Throughout the life of the project, there\\nmay be several different implementations, and within each\\nimplementation, there may be different versions of the core\\nbuilding blocks, but everything we work on should follow\\nthese core principles:\\n•Sensorimotor learning and inference: We use\\nactively generated temporal sequences of sensory\\nand motor inputs instead of static inputs. The\\noutputs of the system are motor commands.\\n•Modular structure: The same algorithm needs\\nto work for all modalities. This general algorithm\\nembodied in a learning module makes the system\\neasily expandable and scalable.\\n•Cortical Messaging Protocol: The inputs and\\noutputs of a learning module adhere to a defined\\nprotocol such that many different sensor modules\\n(and modalities) and learning modules can work\\ntogether seamlessly.\\n•Voting: A mechanism by which a collection of\\nexperts can use different information and mod-\\nels to come to a faster, more robust and stable\\nconclusion.',\n",
              " 'together seamlessly.\\n•Voting: A mechanism by which a collection of\\nexperts can use different information and mod-\\nels to come to a faster, more robust and stable\\nconclusion.\\n•Reference frames: The learned models should\\nhave inductive biases that make them naturally\\ngood at modeling a structured world that evolves\\nover time. The learned models can be used for a\\nvariety of tasks such as manipulation, planning,\\nimagining previously unseen states of the world,\\nfast learning, generalization, and many more.\\n•Rapid, continual learning where learning and\\ninference are closely intertwined: Supported by\\nsensorimotor embodiment and reference frames,\\nbiologically plausible learning mechanisms en-\\nable rapid knowledge accumulation and updates\\nto stored representations while remaining robust\\nunder the setting of continual learning. There\\nis also no clear distinction between learning and\\ninference. We are always learning, and always\\nperforming inference.\\n•Model-free and model-based policies: Low-',\n",
              " 'is also no clear distinction between learning and\\ninference. We are always learning, and always\\nperforming inference.\\n•Model-free and model-based policies: Low-\\nlevel, model-free policies provide efficient means\\nof interacting with the world, but are crucially\\ncombined with model-based policies that support\\nflexible action planning in novel situations.\\nIn the initial implementation presented here, many compo-\\nnents are deliberately notbiologically constrained, and/or\\nsimplified, so as to support visualizing, debugging, and\\nunderstanding the system as a whole. For example, ob-\\nject models are currently based on explicit graphs in 3D\\nCartesian space. In the future, these elements may be sub-\\n3The Thousand Brains Project\\nstituted with more powerful, albeit more inscrutable neural\\ncomponents.\\n2.3 Challenging Preconceptions\\nSeveral of the ideas and ways of thinking introduced in\\nthis document may be counter-intuitive to those familiar\\nwith current AI methods, including deep learning. For',\n",
              " '2.3 Challenging Preconceptions\\nSeveral of the ideas and ways of thinking introduced in\\nthis document may be counter-intuitive to those familiar\\nwith current AI methods, including deep learning. For\\nexample, ideas about intelligent systems, learning, models,\\nhierarchical processing, or action policies that you already\\nhave in mind might not apply to the system that we are\\ndescribing. We therefore ask the reader to try and dispense\\nwith as many preconceptions as possible and to understand\\nthe ideas presented here on their own terms. We are happy\\nto discuss any questions or thoughts that may arise from\\nreading this document. Please join our Discourse forum or\\nreach out to us at info@thousandbrains.org.\\nBelow, we highlight some of the most important differ-\\nences between the system we are building and other AI\\nsystems.\\n•We are building a sensorimotor system. It learns\\nby interacting with the world and sensing differ-\\nent parts of it over time. It does not learn from a',\n",
              " 'systems.\\n•We are building a sensorimotor system. It learns\\nby interacting with the world and sensing differ-\\nent parts of it over time. It does not learn from a\\nstatic dataset . This is a fundamentally different\\nway of learning than most leading AI systems\\ntoday and addresses a (partially overlapping) dif-\\nferent set of problems.\\n•We will introduce learning modules as the basic,\\nrepeatable modeling unit, comparable to a cortical\\ncolumn. An important detail to point out here is\\nthat none of these modeling units receives the full\\nsensory input. For example, in vision, there is\\nno ‘full image’ anywhere . Each sensor senses\\na small patch in the world. This is in contrast to\\nmany AI systems today, where all sensory input\\nis fed into a single model.\\n•Despite the previous point, each modeling sys-\\ntem can learn complete models of objects and\\nrecognize them on its own. A single modeling\\nunit should be able to perform all basic tasks\\nof object recognition and manipulation . Us-',\n",
              " 'tem can learn complete models of objects and\\nrecognize them on its own. A single modeling\\nunit should be able to perform all basic tasks\\nof object recognition and manipulation . Us-\\ning more modeling units makes the system faster\\nand more efficient and supports compositional\\nand abstract representations, but a single learning\\nmodule is itself a powerful system. In the sin-\\ngle module scenario, inference always requires\\nmovement to collect a series of observations, in\\nthe same way that recognizing a coffee cup with\\none finger requires moving across its surface.\\n•All models are structured by reference frames.\\nAn object is not just a bag of features. It is a\\ncollection of features at locations. The relative\\nlocations of features to each other are more\\nimportant than the features themselves . These\\nprinciples are used for modeling all discrete con-\\ncepts in the world, from the simplest of physicalobjects to abstract concepts in society or mathe-\\nmatics.',\n",
              " 'principles are used for modeling all discrete con-\\ncepts in the world, from the simplest of physicalobjects to abstract concepts in society or mathe-\\nmatics.\\n•Action policies are, first and foremost, model-\\nbased . Learned models of objects in the world\\nare used to determine appropriate actions in novel\\nsituations. Any given learning module can use its\\ninternal models to propose goal-states that are ei-\\nther decomposed into simpler goal-states in other\\nlearning modules, or are acted upon directly by\\nmotor systems. In this way, complex policies can\\nbe hierarchically decomposed, while still lever-\\naging learned models. Over time and with prac-\\ntice, model-based policies become more efficient,\\nwhile model-free policies can learn to do certain\\ntasks rapidly and with finesse, but model-free\\npolicies are not the initial basis of actions in unfa-\\nmiliar settings.\\n•Motor output can be generated at any level\\nof the system. In contrast to many current ap-',\n",
              " 'policies are not the initial basis of actions in unfa-\\nmiliar settings.\\n•Motor output can be generated at any level\\nof the system. In contrast to many current ap-\\nproaches for sensorimotor interaction, we do not\\nhave a separate hierarchy of sensory processing\\nfollowed by the generation of motor commands.\\nInstead, each learning module, even at the lowest\\nsensory level, produces action outputs. This is\\nanalogous to the projections to subcortical motor\\nregions found in every area of the neocortex, even\\nregions classically thought of as sensory regions.\\n2.4 Capabilities of the System\\nThe thousand-brains architecture is designed to be a\\ngeneral-purpose AI system. It is not designed to solve\\na specific task or set of tasks. Instead, it is designed to be\\na platform that can be used to build a wide variety of AI\\napplications. Like an operating system or a programming\\nlanguage does not define what the user applies it to, the\\nThousand Brains Project will provide the tools necessary',\n",
              " 'applications. Like an operating system or a programming\\nlanguage does not define what the user applies it to, the\\nThousand Brains Project will provide the tools necessary\\nto solve many of today’s current problems as well as com-\\npletely new and unanticipated ones without being specific\\nto any one of them.\\nEven though we cannot predict the ultimate use cases of\\nthe system, we want to test it on a variety of tasks and keep\\na set of capabilities in mind when designing the system.\\nThe basic principle here is that it should be able to solve\\nany task the neocortex can solve. If we come up with a\\nnew mechanism that makes it fundamentally impossible\\nto do something the neocortex can do, we need to rethink\\nthe mechanism. For example, thousand-brains systems\\nshould be able to model the world through any kind of\\nmovement-based sensory modality, from touch to echolo-\\ncation. They should also be able to conceptualize abstract\\nspaces, execute a series of intricate movements, and plan',\n",
              " 'movement-based sensory modality, from touch to echolo-\\ncation. They should also be able to conceptualize abstract\\nspaces, execute a series of intricate movements, and plan\\nlong-term actions. However, tasks such as multiplying\\narbitrary large numbers, or predicting the structure of a\\nprotein given its genetic sequence, are domains much bet-\\nter left to alternative technologies, such as calculators or\\ndeep-learning.\\n4The Thousand Brains Project\\nThe following is a list of capabilities that we always con-\\nsider when designing and implementing the system. We\\nare not looking for point solutions for each of these prob-\\nlems but a general algorithm that can solve all of them. It\\nis by no means a comprehensive list, but it should give an\\nidea of the scope of the system.\\n•Recognizing objects independent of their location\\nand orientation in the world.\\n•Determining the location and orientation of an\\nobject relative to the observer or to another object\\nin the world.',\n",
              " '•Recognizing objects independent of their location\\nand orientation in the world.\\n•Determining the location and orientation of an\\nobject relative to the observer or to another object\\nin the world.\\n•Recognizing an object and its pose by moving\\none sensor over the object.\\n•Performing flash inference (inference with no\\nmovement) by using many sensors in tandem.\\n•Performing learning and inference under noisy\\nconditions.\\n• Learning from a small number of samples.\\n•Learning from continuous interaction with the\\nenvironment, maintaining previously learned rep-\\nresentations.\\n• Learning without explicit supervision.\\n•Recognizing objects when other objects partially\\nocclude them.\\n•Learning categories of objects and generalizing\\nto new instances of a category.\\n•Learning and recognizing compositional objects,\\nincluding novel combinations of their parts.\\n•Recognizing objects subject to novel deforma-\\ntions (e.g. Dali’s ‘melting clocks’, a crumpled up\\nt-shirt, or recognizing objects learned in 3D but',\n",
              " 'including novel combinations of their parts.\\n•Recognizing objects subject to novel deforma-\\ntions (e.g. Dali’s ‘melting clocks’, a crumpled up\\nt-shirt, or recognizing objects learned in 3D but\\nseen in 2D).\\n•Recognizing an object independent of its scale,\\nand estimating its scale.\\n•Modeling and recognizing object states and be-\\nhaviors (e.g. if a stapler is open or closed; whether\\na person is walking or running, and how their\\nbody evolves over time under these conditions).\\n•Using learned models to alter the world and\\nachieve goals, including goals that require de-\\ncomposition into simpler tasks.\\n•Generalizing modeling to abstract concepts de-\\nrived from concrete models.\\n•Modeling language and associating it with\\ngrounded models of the world.\\n• Modeling other entities (Theory of Mind).\\n3 Overview Of The Architecture\\nThere are three major components that play a role in the\\narchitecture: sensor modules, learning modules, and themotor system. These three elements are tied together by a',\n",
              " '3 Overview Of The Architecture\\nThere are three major components that play a role in the\\narchitecture: sensor modules, learning modules, and themotor system. These three elements are tied together by a\\nfinal key component, a common communication protocol.\\nDue to this unified messaging protocol, the inner workings\\nof each individual component can be quite varied as long\\nas they have the appropriate interfaces. A simple example\\nof a sensor module coupled to a learning module is shown\\nin Figure 1, although we will begin by describing the CMP.\\nFigure 1: Sensor modules receive and process the raw\\nsensory and motor input. This is then communicated via a\\ncommon messaging protocol to a learning module which\\nuses this information to learn and recognize models of\\nanything in the environment.\\n3.1 Cortical Messaging Protocol\\nWe use a common communication protocol that all compo-\\nnents - learning modules, sensor modules, and the motor\\nsystem - use to share information. By defining a consistent',\n",
              " 'We use a common communication protocol that all compo-\\nnents - learning modules, sensor modules, and the motor\\nsystem - use to share information. By defining a consistent\\ninformation format that sensor modules and learning mod-\\nules must output, and that motor systems must receive, it\\nis possible for all components to communicate with each\\nother, and to combine them arbitrarily. Due to its inspira-\\ntion from long-range connections in the cortex, we call this\\ncommon communication protocol the Cortical Messaging\\nProtocol (CMP)\\nIn order to define the CMP, we must first define what we\\nmean by an object, or a feature, in Monty. An object is\\na discrete entity composed of a collection of one or more\\nother objects, each with their own associated pose. For ex-\\nample, an apple at a location and orientation in space is an\\nobject, but equally, an object could be a scene, an abstract\\narrangement of concepts, or any other composition of sub-\\nobjects. At the lowest level of this object hierarchy, an',\n",
              " 'object, but equally, an object could be a scene, an abstract\\narrangement of concepts, or any other composition of sub-\\nobjects. At the lowest level of this object hierarchy, an\\nobject is composed of inputs from a sensor module, which\\nare also discrete entities with a location and orientation in\\nspace. Sensor inputs play a similar role to objects at other\\n5The Thousand Brains Project\\npoints in a hierarchy of learning modules, but the ‘objects’\\ndetected by sensors cannot be further decomposed. Wher-\\never an object is being processed by a component of the\\nsystem, it can also be referred to as a feature . By conven-\\ntion, we usually refer to the input of a learning module\\nas features and the output as an object ID. However, the\\nobject ID output of one learning module can become the\\nfeature input to the next learning module so they are by\\ndefinition the same.\\nAt its core, a CMP-compliant output contains a feature at\\na pose . The pose contains a location in 3D space (naturally',\n",
              " 'feature input to the next learning module so they are by\\ndefinition the same.\\nAt its core, a CMP-compliant output contains a feature at\\na pose . The pose contains a location in 3D space (naturally\\nincluding 1D or 2D space) representing where the sensed\\nfeature is relative to the body, or another common reference\\npoint such as a landmark in the environment. In addition to\\nlocation, the pose includes information about the feature’s\\n3D orientation, which could be defined by the direction\\nof a surface’s point normal and its principal curvature, or\\nthe orientation of an object. Importantly, the message may\\ncontain additional feature information, such as color, the\\nmagnitude of sensed curvature, or an object ID. Counter-\\nintuitively, the nature of the feature does not need to be\\nspecified in the message for it to be a valid signal.\\nWe highlight the choice that non-pose attributes of a fea-\\nture are optional. This is contrary to many existing AI',\n",
              " 'specified in the message for it to be a valid signal.\\nWe highlight the choice that non-pose attributes of a fea-\\nture are optional. This is contrary to many existing AI\\nsystems where models are often closer to bags-of-features\\nand object structure is weakly represented, if at all. Here,\\nthe relative locations of features are more important than\\nthe features themselves. An example of how this aligns\\nwith human perception is how fruits arranged in the shape\\nof a face can be easily recognized as a face, even though\\nno typical face \"features\" are present. On the other hand,\\nhumans would not classify a random arrangement of eyes,\\na nose, and lips as a face.\\nBesides features and their poses, the standardized message\\nalso includes information about the sender’s ID (e.g. the\\nparticular sensor module) and a confidence rating. Fur-\\nther below we discuss the internal models that learning-\\nmodules (LMs) develop - importantly, the CMP is never\\nused to share these models between LMs. Instead, it can',\n",
              " 'ther below we discuss the internal models that learning-\\nmodules (LMs) develop - importantly, the CMP is never\\nused to share these models between LMs. Instead, it can\\nonly communicate more abstract information about these\\nmodels (such as an object ID). The inputs and outputs of\\nthe system (raw sensory input to the SM and motor com-\\nmand outputs from motor modules) can have any format\\nand do not adhere to any messaging protocol. They are\\nspecific to the agents sensors and actuators and represent\\nthe systems interface with the environment.Fin a common\\nreference frame (e.g. relative to the body2). This makes it\\npossible for all components to meaningfully interpret the\\npose information they receive.\\n2In the following sections we may call this common reference\\nframe \"body-centric\" . In general, we just mean a common refer-\\nence frame for all sensors. There may be applications without a\\nconcrete body (like several cameras set up in different locations',\n",
              " 'frame \"body-centric\" . In general, we just mean a common refer-\\nence frame for all sensors. There may be applications without a\\nconcrete body (like several cameras set up in different locations\\nof a room, a swarm of agents, or an agent navigating a more ab-\\nstract space like the internet) where this just refers to an arbitrary\\npoint in space that all communicated poses are relative to.3.2 Sensor Modules\\nThousand-brains systems can work with any type of sensor\\n(vision, touch, radar, LiDAR,...) and integrate information\\nfrom multiple sensory modalities without effort. For this\\nto work, sensor modules need to communicate information\\nin a common language. Transforming raw sensory input\\ninto this common language is the job of the sensor module.\\nEach sensor module receives information from a small\\nsensory patch as input. This is analogous to a small patch\\non the retina, or a patch of skin, or the pressure information\\nat one whisker of a mouse. In the simplest architecture, one',\n",
              " 'sensory patch as input. This is analogous to a small patch\\non the retina, or a patch of skin, or the pressure information\\nat one whisker of a mouse. In the simplest architecture, one\\nsensor module sends information to one learning module,\\nwhich models this information. How such local sensory\\ninputs are integrated across time and space will be covered\\nin a moment when we discuss learning modules.\\nThe information processing within the sensor module turns\\nthe raw information from the incoming sensor patch into\\nthe cortical messaging protocol (detailed in section 3.1).\\nThis process can be compared to light hitting the retina\\nand being converted into spikes, the output of biological\\nneurons. Additionally, the pose of the feature relative to\\nthe body is calculated from the feature’s pose relative to the\\nsensor and the sensor’s pose relative to the body. As such,\\neach sensor module outputs the feature it senses, as well as\\nthe feature’s pose (location and rotation) in body-centric',\n",
              " 'sensor and the sensor’s pose relative to the body. As such,\\neach sensor module outputs the feature it senses, as well as\\nthe feature’s pose (location and rotation) in body-centric\\ncoordinates. The availability of this pose information is\\ncentral to how the thousand-brains architecture operates.\\nA general principle of the system is that any processing\\nspecific to a modality happens in the sensor module. The\\noutput of the sensor module is not modality-specific any-\\nmore and can be processed by any learning module. A\\ncrucial requirement here is that each sensor module knows\\nthe pose of the feature relative to the sensor. This means\\nthat sensors need to be able to detect features and poses of\\nfeatures. They also need to be able to keep track of their\\nposition in space. This could be directly provided from the\\nsystem, inferred from sensory inputs (like optical flow), or\\ncalculated from efference copies of motor commands.\\n3.3 Learning Modules',\n",
              " 'position in space. This could be directly provided from the\\nsystem, inferred from sensory inputs (like optical flow), or\\ncalculated from efference copies of motor commands.\\n3.3 Learning Modules\\nThe basic building block for sensorimotor processing and\\nmodeling is the learning module (LM). These are repeating\\nelements, each using the same input and output interface.\\nEach LM should function as a stand-alone unit and be able\\nto recognize objects on its own. Combining multiple LMs\\ncan speed up recognition (e.g. recognizing a cup using five\\nfingers vs. one), allows for LMs to focus on storing only\\nsome objects, and enables learning compositional objects.\\nLMs receive features at poses. Features can either be\\nfeature IDs from a sensor module or object IDs (also in-\\nterpreted as features) from a lower-level LM. The feature\\nor object representation might be in the form of a discrete\\nID (e.g. the color red, a cylinder), or could be represented',\n",
              " 'terpreted as features) from a lower-level LM. The feature\\nor object representation might be in the form of a discrete\\nID (e.g. the color red, a cylinder), or could be represented\\nin a more high dimensional space (e.g. a vector of binary\\nvalues representing hue, or corresponding to a fork-like\\n6The Thousand Brains Project\\nFigure 2: Learning modules learn structured models\\nthrough sensorimotor interaction, using reference frames.\\nThey model how incoming features are arranged relative\\nto each other in space.\\nobject). Additionally, LMs receive the feature’s or object’s\\npose relative to the body, where the pose includes location\\nand rotation. In this way, body-centric coordinates serve\\nas a common reference frame for spatial computations, as\\nopposed to the pose of features relative to each individual\\nsensor. From this information, higher-level LMs can build\\nup structured models of compositional objects (e.g. large\\nobjects or scenes).\\nThe features and relative poses are incorporated into a',\n",
              " 'sensor. From this information, higher-level LMs can build\\nup structured models of compositional objects (e.g. large\\nobjects or scenes).\\nThe features and relative poses are incorporated into a\\nmodel of the object. All models have an inductive bias\\ntowards learning objects within a 3-dimensional space,\\ncomplimented by a temporal dimension. When interacting\\nwith the physical world, the 3D inductive bias is used to\\nplace features in internal models accordingly. However,\\nthe exact structure of space can potentially be learned,\\nsuch that the lower-dimensional space of a melody, or the\\nabstract space of a family tree, can be represented.\\nThe LM, therefore, encompasses two major principles of\\nthe TBT: sensorimotor learning, and building models using\\nreference frames (see Figure 2). Both ideas are motivated\\nby studies of cortical columns in the neocortex (see Figure\\n3), as well as Hawkins et al. [2017, 2019].\\nBesides learning new models, the LM also tries to match',\n",
              " 'by studies of cortical columns in the neocortex (see Figure\\n3), as well as Hawkins et al. [2017, 2019].\\nBesides learning new models, the LM also tries to match\\nthe observed features and relative poses to already learned\\nmodels stored in memory. Internally, LMs use displace-\\nments between consecutive poses and map them into the\\nmodel’s reference frame. This makes it possible to detect\\nobjects even at novel poses.To generate the LM’s output , we need to get the pose of\\nthe sensed object relative to the body. We can calculate\\nthis from the current incoming pose (pose of the sensed\\nfeature relative to the body) and the poses stored in the\\nmodel of the object. This pose of the object can then be\\npassed hierarchically to another LM in the same format as\\nthe sensory input (features at a pose relative to the body\\nwhere the feature is the inferred object ID).\\nOnce the LM has determined an object’s ID and pose, it\\ncan use the most recent observations (and possibly collect',\n",
              " 'where the feature is the inferred object ID).\\nOnce the LM has determined an object’s ID and pose, it\\ncan use the most recent observations (and possibly collect\\nmore) to update its model. As such, LMs continually learn\\nmore about the world, and learning and inference are two\\nclosely intertwined processes.\\n3.4 Motor Information and Action Policies\\nMovement is central to how thousand-brains systems un-\\nderstand the world. The spatial nature of reference frames\\nis dependent on integrating movement information so that\\na learning module knows where its input features are lo-\\ncated at any given moment. The movement information\\n(pose displacement) can be a copy of the selected action\\ncommand (efference copy) or deduced from the sensory in-\\nput. Without the efference copy, movement can be detected\\nfrom information such as optical flow or proprioception.\\nSensor modules use movement information to update their\\npose relative to the body. LMs use it to update the hypothe-',\n",
              " 'from information such as optical flow or proprioception.\\nSensor modules use movement information to update their\\npose relative to the body. LMs use it to update the hypothe-\\nsized location of their incoming features within an object’s\\nreference frame.\\nWhile movement is clearly important for an LM to un-\\nderstand the outside world, it is also important that this\\nmovement is not random. What’s more, an intelligent sys-\\ntem should be able to exert influence on the external world.\\nThis is where policies become crucial.\\nThousand-brains systems make use of a combination of\\nmodel-free policies, corresponding to lower-level compo-\\nnents of the system (sensor-module - motor-system loops),\\ntogether with model-based policies based within LMs and\\nusing learned models to inform actions.\\nModel-based policies use more computational resources\\nto enable more principled movement, such as moving a\\nsensor to a location that will minimize uncertainty about',\n",
              " 'Model-based policies use more computational resources\\nto enable more principled movement, such as moving a\\nsensor to a location that will minimize uncertainty about\\nthe currently observed object. These policies are derived\\nfrom LMs, where each LM produces a motor output, anal-\\nogous to the universal motor outputs found in cortical\\ncolumns [Prasad et al., 2020]. The motor output is for-\\nmalized as a goal state and also adheres to the CMP. The\\ngoal state could, for example, use the learned models and\\ncurrent hypotheses to calculate a sensor state that resolves\\nuncertainty about which of two possible objects is being\\nobserved. It can also help to guide directed and more ef-\\nficient exploration of parts of objects that are currently\\nunderrepresented in the internal models. Different poli-\\ncies can be leveraged depending on whether we are trying\\nto recognize an object or trying to learn new information\\nabout an object. Finally, policies can enable a learning\\n7The Thousand Brains Project',\n",
              " 'cies can be leveraged depending on whether we are trying\\nto recognize an object or trying to learn new information\\nabout an object. Finally, policies can enable a learning\\n7The Thousand Brains Project\\nFigure 3: Conceptual sketch of how the learning module could be implementing possible mechanisms of cortical\\ncolumns. The figure on the right represents three cortical columns, including cellular layers. The internal structure of a\\nlearning module can be mapped onto these layers.\\nmodule to change the state of the world, such as pushing a\\nbutton, or changing the position of an object on a table.\\nHierarchy can also be leveraged for goal-states, where a\\nmore abstract goal-state in a high-level LM can be achieved\\nby decomposing it into simpler goal-states for lower-level\\nLMs. Importantly, the same LMs that learn models of ob-\\njects are used to generate goal-states, enabling hierarchical,\\nmodel-based policies.\\nModel-free policies are useful for purely sensory-based ac-',\n",
              " 'LMs. Importantly, the same LMs that learn models of ob-\\njects are used to generate goal-states, enabling hierarchical,\\nmodel-based policies.\\nModel-free policies are useful for purely sensory-based ac-\\ntions such as smoothly moving a sensor over the surface of\\nan object, or attending to a prominent feature. Model-free\\npolicies can also learn to carry out frequently performed\\ntasks in a dexterous and rapid manner, freeing computa-\\ntional resources required for model-based policies. Finally,\\ngoal-states generated by LMs must be transformed into\\nmotor commands for actuators - a process that recruits\\nmodel-free policies (innate or learned) in the motor sys-\\ntem.\\nIn the brain, much of this processing occurs subcortically.\\nIn a thousand-brains system, this corresponds to the motor-\\nsystem area. We note that the motor area does not know\\nabout models of objects that are learned in the LMs, and\\ntherefore needs to receive useful goal states from the LMs.',\n",
              " 'system area. We note that the motor area does not know\\nabout models of objects that are learned in the LMs, and\\ntherefore needs to receive useful goal states from the LMs.\\nThese commands adhere to the CMP, but the outputs of\\nthe motor area will deviate from the protocol in order to\\ninterface with the actuators of the system. This means the\\nmotor system serves the reverse role of the sensor module,\\ntranslating CMP-compliant goal states into the specific\\nmovement commands of the actuator it is connected to.\\n3.5 Multi-LM Systems\\nAny given Monty system can be composed of multiple\\nlearning modules. Depending on their arrangement, LMsinteract with one-another in a hierarchical manner, or via\\nvoting. A brief overview of these concepts is given below,\\nwhile these possibilities are shown visually in Figure 4.\\n3.5.1 Hierarchy: Composition and Learning on\\nDifferent Spatial Scales\\nLearning modules can be stacked in a hierarchical fashion\\nto process larger input patches and higher-level concepts.',\n",
              " '3.5.1 Hierarchy: Composition and Learning on\\nDifferent Spatial Scales\\nLearning modules can be stacked in a hierarchical fashion\\nto process larger input patches and higher-level concepts.\\nA higher-level LM receives feature and pose information\\nfrom the output of a lower-level module and/or from a\\nsensor patch with a larger receptive field, mirroring the\\nconnectivity of the cortex. The lower-level LM never sees\\nthe entire object it is modeling at once but infers it either\\nthrough multiple consecutive movements and/or voting\\nwith other modules. The higher-level LM can then use the\\nrecognized model ID as a feature in its own models. This\\nmakes it more efficient to learn larger and more complex\\nmodels as we do not need to represent all object details\\nwithin one model. In particular, this enables the repre-\\nsentation of compositional objects by quickly associating\\ndifferent object parts with each other as relative features\\nin a higher-level model. We discuss the importance of',\n",
              " 'sentation of compositional objects by quickly associating\\ndifferent object parts with each other as relative features\\nin a higher-level model. We discuss the importance of\\ncomposition more later.\\n3.5.2 Voting: Rapid Consensus\\nLMs share lateral connections in order to communicate\\ntheir estimates of the current object ID and pose. This\\nprocess, which we term voting , adheres to the CMP, pass-\\ning feature-pose information. Unlike connections between\\nlower and higher LMs however, voting communicates a set\\nof all possible objects and poses under the current evidence\\n(i.e. multiple messages adhering to the CMP). Through the\\nlateral voting connections, LMs attempt to reach a consen-\\nsus on which object they are sensing at the moment and its\\n8The Thousand Brains Project\\nFigure 4: By using a common messaging protocol between sensor modules and learning modules, the system can easily',\n",
              " '8The Thousand Brains Project\\nFigure 4: By using a common messaging protocol between sensor modules and learning modules, the system can easily\\nbe scaled in multiple dimensions. This provides a straightforward way for dealing with multiple sensory modalities.\\nUsing multiple learning modules next to each other can improve robustness and speed through votes between them.\\nAdditionally, stacking learning modules on top of each other allows for more complex, hierarchical processing of inputs\\nand modeling compositional objects.\\n9The Thousand Brains Project\\npose. This helps to recognize objects faster than a single\\nmodule could.\\nWe earlier highlighted that CMP messages are encoded in a\\ncommon reference frame. This is key for voting to account\\nfor the relative displacement of sensors and, therefore,\\nlocations within LM models. For example, when two\\nfingers touch a coffee mug in two different parts, one might\\nsense the rim, while the other senses the handle. As such,',\n",
              " 'locations within LM models. For example, when two\\nfingers touch a coffee mug in two different parts, one might\\nsense the rim, while the other senses the handle. As such,\\n‘coffee mug’ will be in both of their working hypotheses\\nabout the current object. When voting, they do not simply\\ncommunicate ‘coffee mug’, but also where on the coffee\\nmug other LMs should be sensing it, according to their\\nrelative displacements. As a result, voting is not simply a\\n‘bag-of-features’ operation but is dependent on the relative\\narrangement of features in the world.\\nNote that votes sent via the CMP do not contain any infor-\\nmation about the input features received by that LM. For\\nexample, an LM might receive point-normals and surface\\ncurvature as its input features from an SM, and use this to\\nmodel objects like coffee mugs and staplers. During voting,\\nit will communicate its hypotheses around coffee mugs and\\nstaplers, but it will not communicate any information about']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD3zHRGVh4lZ"
      },
      "source": [
        "Load the dataset into the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6wcytlegi-D",
        "outputId": "ffc42afd-4127-41d6-b352-d13247bc9d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 50 headlines.\n"
          ]
        }
      ],
      "source": [
        "astra_vector_store.add_texts(texts)\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "astra_vector_index=VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGVZTUgoiwzD"
      },
      "source": [
        "Run the QA cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zADSkOvqi5RD"
      },
      "source": [
        "What is the Thousand Brains Project, and how does it differ from traditional deep learning approaches?\n",
        "How is the concept of cortical columns in the neocortex integrated into the Thousand Brains Project's design?\n",
        "What are the core principles guiding the Thousand Brains Project, and how do they reflect neuroscientific insights?\n",
        "What is the Cortical Messaging Protocol (CMP), and why is it significant in the system's architecture?\n",
        "### Technical Aspects\n",
        "What role do learning modules play in the Thousand Brains architecture, and how do they function independently and collaboratively?\n",
        "How does the Thousand Brains Project handle sensorimotor learning, and why is this approach fundamental?\n",
        "What are reference frames, and how are they utilized for structuring models within the system?\n",
        "Can you explain the hierarchy and voting mechanisms in multi-learning module systems?\n",
        "### Implementation\n",
        "What are the capabilities of the Monty system as the first instantiation of the Thousand Brains Project?\n",
        "How does the system achieve rapid and continual learning, and what are the challenges involved?\n",
        "How does the system ensure generalization across different sensory modalities and task domains?\n",
        "### Evaluation and Applications\n",
        "What types of experimental evaluations have been conducted for the Monty system, and what are the results?\n",
        "How does the system apply its learned models to interact with and manipulate the environment?\n",
        "What are some potential real-world applications of the Thousand Brains architecture?\n",
        "### Philosophical and Long-Term Considerations\n",
        "How does the Thousand Brains Project challenge preconceptions about AI and machine learning?\n",
        "In what ways does the project aim to replicate human-like intelligence, and where does it deliberately diverge?\n",
        "### Future Directions\n",
        "What improvements or advancements are planned for the next iterations of the Thousand Brains architecture?\n",
        "How does the project envision integrating more biologically accurate models, such as neural grid cells, into the system?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjfMfL5Dh6H6",
        "outputId": "1b0bd10e-b771-4c15-b26a-b81e1f4b439d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your question (or type 'quit' to exit): What are the capabilities of the Monty system as the first instantiation of the Thousand Brains Project?\n",
            "\n",
            "QUESTION: \"What are the capabilities of the Monty system as the first instantiation of the Thousand Brains Project?\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: \"Monty, the first instantiation of the Thousand Brains Project, is capable of embodying key principles of the Thousand Brains Theory and using those principles to enable efficient learning of generalizable representations from sensorimotor data. It is also designed to support multimodal integration and the development of more abstract representations. In addition, Monty is capable of interacting with its environment using several different sensors and motor systems.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [0.9561] \"cies. Building on these core concepts, we described Monty,\n",
            "the first instantiation o ...\"\n",
            "   [0.9479] \"environment using several different sensors, in this case,\n",
            "touch and vision.\n",
            "While p ...\"\n",
            "   [0.9459] \"intelligent, more flexible, and more capable in the many\n",
            "applications that deep lear ...\"\n",
            "   [0.9448] \"biases around the spatial structure of the world to enable rapid and continual learn ...\"\n",
            "\n",
            "Enter your question (or type 'quit' to exit): What is the Cortical Messaging Protocol (CMP), and why is it significant in the system's architecture?\n",
            "\n",
            "QUESTION: \"What is the Cortical Messaging Protocol (CMP), and why is it significant in the system's architecture?\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: \"The Cortical Messaging Protocol (CMP) is a common communication protocol used by all components in the architecture - sensor modules, learning modules, and the motor system - to share information. It is significant because it allows for the seamless integration and communication between different components, as long as they have the appropriate interfaces. This allows for flexibility and versatility in the system, as different components can have varied inner workings as long as they can communicate through the CMP. The CMP is also inspired by long-range connections in the cortex, making it an effective and efficient communication protocol. Overall, the CMP plays a crucial role in the architecture by enabling the integration and coordination of different components to achieve the system's goals.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [0.9293] \"3 Overview Of The Architecture\n",
            "There are three major components that play a role in  ...\"\n",
            "   [0.9278] \"We use a common communication protocol that all compo-\n",
            "nents - learning modules, sen ...\"\n",
            "   [0.9112] \"vature, etc.\n",
            "• ‘Confidence’ (defined in the range [0, 1]).\n",
            "•A boolean for whether th ...\"\n",
            "   [0.9051] \"blocks. The architecture we are creating is built on this\n",
            "premise. Thousand-brains s ...\"\n",
            "\n",
            "Enter your question (or type 'quit' to exit): What is the primary computational unit in the Thousand Brains Project?\n",
            "\n",
            "QUESTION: \"What is the primary computational unit in the Thousand Brains Project?\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: \"The primary computational unit in the Thousand Brains Project is the cortical column, as proposed by Vernon Mountcastle's theory of intelligence. This idea is also reflected in the first practical implementation of a thousand-brains system called \"Monty\".\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [0.9402] \"∗Joint first authors.arXiv:2412.18354v1  [cs.AI]  24 Dec 2024The Thousand Brains Pro ...\"\n",
            "   [0.9379] \"blocks. The architecture we are creating is built on this\n",
            "premise. Thousand-brains s ...\"\n",
            "   [0.9363] \"intelligent, more flexible, and more capable in the many\n",
            "applications that deep lear ...\"\n",
            "   [0.9336] \"THETHOUSAND BRAINS PROJECT : A N EWPARADIGM FOR\n",
            "SENSORIMOTOR INTELLIGENCE\n",
            "Viviane Cl ...\"\n",
            "\n",
            "Enter your question (or type 'quit' to exit): What is the term used for the shared coordinate system within the Thousand Brains Project?\n",
            "\n",
            "QUESTION: \"What is the term used for the shared coordinate system within the Thousand Brains Project?\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: \"The shared coordinate system within the Thousand Brains Project is called the \"Cortical Messaging Protocol\" or CMP.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [0.9273] \"∗Joint first authors.arXiv:2412.18354v1  [cs.AI]  24 Dec 2024The Thousand Brains Pro ...\"\n",
            "   [0.9240] \"8The Thousand Brains Project\n",
            "Figure 4: By using a common messaging protocol between  ...\"\n",
            "   [0.9235] \"biases around the spatial structure of the world to enable rapid and continual learn ...\"\n",
            "   [0.9233] \"blocks. The architecture we are creating is built on this\n",
            "premise. Thousand-brains s ...\"\n",
            "\n",
            "Enter your question (or type 'quit' to exit): quit\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "      query_text=input(\"\\nwhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower()==\"quit\":\n",
        "      break\n",
        "    if query_text==\"\":\n",
        "      continue\n",
        "\n",
        "    firsy_question=False\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer=astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(\"\\nANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text,k=4):\n",
        "      print(\"   [%0.4f] \\\"%s ...\\\"\" % (score,doc.page_content[:84]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAP5EUnTlGKE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
